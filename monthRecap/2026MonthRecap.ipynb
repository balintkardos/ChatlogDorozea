{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a61989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c96ab",
   "metadata": {},
   "source": [
    "show cout 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58865add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of filenames from the configuration file\n",
    "with open('../file_list.txt', 'r', encoding='utf-8') as config_file:\n",
    "    file_names = config_file.read().splitlines()\n",
    "\n",
    "# Regex pattern to match the data format\n",
    "pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "\n",
    "# Initialize an empty list to store parsed data\n",
    "datalist = []\n",
    "stream_count = 0\n",
    "# Iterate over each specified file\n",
    "for file in file_names:\n",
    "    full_path = f\"../data/{file}\"\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, user, message = match.groups()\n",
    "                datalist.append([date, user, message,stream_count])\n",
    "    stream_count = stream_count + 1\n",
    "\n",
    "# Create a DataFrame from the parsed data\n",
    "data = pd.DataFrame(datalist, columns=[\"date\", \"user\", \"message\",\"stream\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eaac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55691696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_utc_to_cet(df, date_column='date'):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamps to Central European Time (CET/CEST) with proper DST handling\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the date column\n",
    "    date_column (str): Name of the column containing UTC timestamps\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with converted timestamps\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure timestamps are UTC aware\n",
    "    if df[date_column].dt.tz is None:\n",
    "        df[date_column] = df[date_column].dt.tz_localize('UTC')\n",
    "    elif df[date_column].dt.tz != pytz.UTC:\n",
    "        df[date_column] = df[date_column].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Convert to CET/CEST (Europe/Berlin includes proper DST handling)\n",
    "    df[date_column] = df[date_column].dt.tz_convert('Europe/Berlin')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5700f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_utc_to_cet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3b966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"user\"] = data[\"user\"].replace(\"Banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"CHILI_POE\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_conbacon\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Wirelesss_\", \"W1r3lesss\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"treklul\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"ttrek_\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"trek_x\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"TriplesingleJ\", \"TripleSingleJames\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uwu_cougar\", \"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uuccugr_\",\"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"StanIV4_\", \"stan_iv4\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Muuskie2\", \"Muuskie\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"nishad_more1311\", \"nishad13\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballt\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballtt23\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"lajosbarnabas\", \"lajoss__\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Bonkwiththefunk\", \"bonk67\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"qfishyy11\", \"bonk67\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ae4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all unique usernames\n",
    "unique_users = data['user'].unique()\n",
    "\n",
    "# Create a mapping from lowercase username to all variants\n",
    "\n",
    "user_variants = defaultdict(set)\n",
    "for user in unique_users:\n",
    "    user_variants[user.lower()].add(user)\n",
    "\n",
    "# Find usernames with different capitalization\n",
    "duplicate_users = {k: v for k, v in user_variants.items() if len(v) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32df7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from all variants to the canonical (sorted first) variant\n",
    "variant_map = {}\n",
    "for variants in duplicate_users.values():\n",
    "    sorted_variants = sorted(variants)\n",
    "    canonical = sorted_variants[0]\n",
    "    for v in variants:\n",
    "        variant_map[v] = canonical\n",
    "\n",
    "# Replace usernames in 'user' column\n",
    "data['user'] = data['user'].apply(lambda u: variant_map.get(u, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae0e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique emotes: 1040\n"
     ]
    }
   ],
   "source": [
    "# 7tv list\n",
    "tv7_emotes = \"\"\"omeFaded nonono wideSpeedFear wideSpeedDesert SpeedOmg Disappointed ME? ome13 doroPfft CONFUSED doro52 wideSpeedConcerned doroBANGER doroVibe DoroBrainPLug NAHHHH intrigued oda WOOF BANG peepoFirework Gladge omeMoji SobBounce belka doroHips dorochan doroXi DOMEYES DOWAY FRENCH YeahIGetIt doroPls Offline sheMightBeRight 89 dwerk wideGkeyDance3 76 xqcCheer omeCheer Dmile zoro LMAO iguessbro doroScary haii SON omeBru wideSpeedCall plink laracroft ADHD yipe doroWaiting 67 FOWL doroLOL OfCourse tyler67 THYME peepoLost speed61 GOODMOOD Jercula ILOVECS CLOWN thatsCrazy xqcBleh omEING wideSpeedLaugh15 WHATTHEHELI RickyDicky jonkler wideSpeedPumpkin wideSpeedLaugh16 SPIDER chillCat LEEKED soblaugh BANANA wideSpeedLaugh21 Binoculars footstepmenace ome99 doroBanger GhankYou NO YES auntiePls OMEGADANCEBUTGHAST gkeyRide GENERATINGGODSEED perimeter sideeye omePfft MySunshine posture ??? twocatsfightingonacouch dSmile wideSpeedLaugh4 flirtt Suspicion zigzag YeatCat nom omeSad shomonting thinking agafat FirstTimeClanka DORVIS gasp RISING dd uncPLS domgBruh Deafge OMEGADANCEBUTFASTER kim3 Buggin speed8 .... d32 ome54 ohok minionBike Clown Explosion hackingCD JermaSoy MathTime MoneyRain PokiShare TakingNotes :0 :3 :33 :tf: !boost +1 0pixel 1DLove 3Head 3Heading 4House 4Shrug 4Weird 5Head AAAA Acknowledged ACTINUP ADHD agahi AIM AIRBALL AIWITHTHEBRAIDS Alarm ALE Alfred AlienDance AlienPls AlienPls2 AlienPls3 Aloo Alright amongE ANGRE ANOTHERONE Ant AREYOUAGIRL AREYOUAGIRLFtxQcYellingAtYou areyoufr AreYouSeriousRightNeow arnoldHalt arthur Assept AURA AwHellNah Aware AWOO AWOOGA axelF ayo bah BAND Banger BANGER banties Barack barryArrive Barry63 BantiesPaulBeef Based BASED BatChestAbove batman Batman batJAM batPls Beatles Bedge BEG BEGGING Bello BigD bieberDougie BELIEVERS BibleThump BINGO Bleh Bloons BOOBA bog BOOM BOOMIES BOINK BORGIR Borfday brbToilet Broadcaster brb Bruh BRUHMM bruv buh buhbye buhFlipExplode BUSSIN BUSSERS bye CanIHaveADollar cannySilly catAsk catBusiness CatCozy catDespair catEat catJAM catKiss catPls catSigh catSmash CatTime catTwerk CAUGHT Caught CaughtIn4K Celebrating CHADDING characterSelected CHATTERS chatting cheerleaders ChillGuy Chillin Cinema chilling clappi Clap classic CLEAN Clueless CLIPPERS CLOWNDETECTED COCKA cokeBreak COMEHERE Concerned Considering Cooked COPIUM crabPls Crunch CS2 Cuck Cuh D: damily Damn dansi dash Dave deadassFaint Delusional DemonTiming Dentge despair DespairRyan Devious DIESOFCRINGE Dime DinkDonk Dinema doggoSlava dogJAM DogLookingWickedAndCool doid dojaPls dome44 dome32 donowall donoWall doroAunt doroBleh dorobubu doroCD DoroCheer doroFiddy doroFlex doroGHOST doroHEAD doroKick doroL doroMAD doroPray doroRip doroSoy DoroTalkingAgain Dorozea doster DOUBTERS DRAIN Drake DRAMA dreamwastaken drooling drukiDnace drukiDnace2 duaKiss dudWhat EDGE EDM EDITING emo erinNya essaying ewphop eww EZ EZdodge Exerpas Explosion eyeroll fadedthanaho FARMING FeelsBadMan FeelsDankMan FeelsBlackScreen FeelsGladMan FeelsLagMan FeelsLateMan FeelsOkayMan FeelsStrongMan FeelsTiredMan FeelsWeirdMan FeelsWowMan fein FEINFEINFEINFEINFEINFEINFEINFEI FellOff fembajJAM Fiddy FiddyWtf FINALLY firewriting FirstTime FirstTimeBackseating FirstTimeChadder FirstTimeChatter FirstTimeEmoteFail FirstTimeGooner FirstTimePepega FirstTimeTest firsttimebuh FLASHBANG flightnotL Flirt Flushed fnaf footstep forsenCD forsenLaughingAtYou ForsenSingingAtYou forsenPls fortnite fr freakbob freakyfredday freddy Freedom FUNNY g32 GAGAGA gachiGASM gachiHYPER gamily GAMBA GameplayTime GAMING GatieG Gaught GENIUS GetALoadOfThisGuy gg GIGACHAD GIGACHAIR GIGACLAUS GIGAMODS GIGAMOD gigl gkeyFlip gkeyPregnantBounce gkeySMP gkeyUwu gkeywide gkeyWiding gkitten GivenUp girlBoss gkitten glorpaga glorpdetective glorp GlorpMeeting glorprave gmoney goaler goat goblin44 GODDID Gogging GoodBye Gooner gooner GoodTake GOONING GotCaughtTrolling GotEEM gothKiss gPls greetingsladies GREEDY GROOTING GRRR GULP GuitarTime GYAT HABIBI hackingCD HACKERMANS hai HAH HaltEinfachDeineFresseDuHurensohn HandsUp happi HARAM HarryStylesKiss Headbang healed HECOOKING heh HEHE HEHEHEHA Heisenberj HELLO HELP helvete Herewego hesRight heyywithrizz HEYYY hi hiii hiiii Hmm HOBBY HOLY HolyFuck homelessPOV HowDoWeTellHer HowDoWeTellHim hue HUH HUHHHHHHHHHH iAsked ICANT idiot iDrive IFISPEAK IfYouCantSeeThisEmoteUseExclamationMark7tv Ignored IGON imback IMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGET ImNotOk ImtiredBoss INTENSEGAMING islandboy ISeeYou itsover itstime Jackass jacob1 jacob2 jacob3 jacob4 jah Jammies JARVIS Jay JermaSoy jiggy job JOB Joel joever john Johnporkiscalling JokerHAHA JokerLaugh juh JumpScared JUMPSCARE JustAChillGuy JustAnotherDay JustHowItIs justinbieber KaiCenatOhiogyatwithskibiditoiletwatchingtheWrizzhappeningrightinfrontofhimwithfanumtaxtaxingthegyat KanyeStare KEKW KENOUGH KeyShaker kim3 kittyBANGER kittyBop KKalinka KKonaW KKool kratos Lamonting LastTimeChatter lava lebronArrive lebronJAM lebronTROLL LEBRONNN lemon Lemon LetsBingo LETHERCOOK LETSFUCKINGJOE LETSGO LieMeter life Life Listening LiterallyMe Lithuanian LittleTrolling LiveReaction LL LMAOFREAKY lmao Loading LOCKIN lockedin LOL Looking LookUp lore luh lurkk luton LULE LULW MAJ Madge ManchesterUnited Massive? MarblesTime Martin matSad maxwin MeRN me: MeWhenIBuyEgyptianProperty MEGALUL mee Memories merch mhm MicTime mikuPls mindloud modCheck ModAbuse MODDING Modding mods MODS Mog monakS monday monkeyListening monkeySip MONKA monkaTOS monkaW MONKE MinionHoting MoneyRain muted mutted MUGA MVP MVPOfFarallah MYHEARTILOVEDHER myIQ MYLIFE NAILS NAILSING NAHH NAHHH NAHHHH nananAYAYA NAUR NAvsEU Nerd niceguy NOCHECKMARKS NODDERS NOIDONTTHINKSO NoMaidens NOOPERS NOOOOO NOHORNY noonecares NOSHOT NOTED notListening notxqcL NOW NOWAY NOWAYING np nt nuhuh nyehehehe nyanPls nya o7 Ogre ohhh ohhhhhhhhh OHMYGAWDD ohneFinger ohno ohSHIT oj OK Okei okak OLDWORK OM om omE ome10 ome101 ome104 ome105 ome14 ome15 ome18 ome21 ome29 ome32 ome4 ome41 ome44 ome44444444 ome47 ome5 ome51 ome52 ome55 ome57 ome67 ome69 ome79 ome808 ome83 ome9 ome90 ome96 ome99 OMEGADANCE OMEGALUL OMEGALULiguess omEE omeJAM omeJudging omeOhSHIT omeScrajj omeStare OMEYES omeWiggle OMFG omgBruh ongang OneGuy ONEMORE OnMyWayToDoroMomHouse OOOO oopsie otag OuttaPocket OVERWATCH OVERWORKING OverwhelminglyWholesome owoCheer PagBounce PagChomp PagMan Panam parasocial Parasocial PARASOCIAL paris paul Paul paulNya PauseMan PAUSENEMOGU Peace PEEPEES peepoAds peepoBox peepoBelievers peepoClap peepoComfy peepoDJ peepoDoubters peepoEvil peepoFarmer peepoFat peepoGiggles peepoHappy peepoHey peepoHug peepoKiss peepoLeave peepoLegs peepoLove peepoMarch peepoPls peepoPride peepoRiot peepoSad peepoShy peepoSmile peepoStop peepoTalk pepeAgony pepeGun PepeHands pepeJAM PepeLaugh pepePoint PepePls pepeW Pepega PepegaAim PepegaChat PepegaReading PepoG Petter Pffttt Pffttt2 phew phpk pickle PianoTime Pipege pKitten pL Please pleading plink-182 plinkVibe plonk pmo Plotge PogO PogU pointless pokiFlirt pol POLICE Pondering popipopipipopipo poroPls POVbornbefore2000 ppHop ppL ppOverheat Prayge prePffttt PRIMERS PTSD pulNya PuzzleTime qq ragebait RAGEY RAHH RainTime RAMBOLMG RareParrot ratomilton RaveDance RaveTime ratio Reacting RealForsen ReallyMad RebeccaBlack Reddit RememberTheDays RibertJam RiddleMeThis RIPBOZO RIRI Rizzler RobertJam ROFL RoxyPotato RUNNING rt ryanArrive Sadding Sadge SADge SAJ SAVEME SCATTER saythatagain scawy SCHEISSE SCHIZO SCRAPETHATSHITJOHNNY SCHTOP sdd SERIOUSLY SEXO shogaNya Shits shutup Shruge silliness sisyphus Sippin Sits skip SLAY Sleepo Smile smh Smoge SmurfHey Smurfing SNACKING SNEAK SNIFFA sob SOLARFLARE songbird sotruebestie SOYSCREAM Speechless speed1 speed2 speed21 speed25 speed32 speed4 speed44 speed8 speed88 speedVibe spfLEAN:()wiltee_()tonyhawkproskater4:-:-:_FREEWAVE3-encinoman--:enteringwalmart:-wheezethelean-123 SpeedLaugh SpeedLeft SpeedR spongePls squadHips Stare Staring steve Steve SteerR StreamEnding STREAMER STREAMERSGIVINGTHEWORSTFUCKINGTAKESINEXISTENCE StoryTime Surfing SurE sus susDog Susge SUSSY Swag swagJAM ta tak TakingNotes TeamEDWARD test THATHIT ThatsJustMe ThePaulers TheVoices TheWolfInMe Thinking Thinking2 ThisChat ThisIsMinecraft TIMEOUT Tomfoolery totallylistening TriJam TriKool TriSad TRUEING TRIVSsorry ts Tuckge tuff TWEAK typeshit typhu UGH um UltraMad unibrow unemployment unmod uwu uuh VALORANT veryDoro VeryKey VeryPog VeryPogftxQcInTheShower vibePls VibePls VIDEOGAME VIEWERS vips Voices wade Waddup waga wah waiting Waiting WAHHH WAJAJA WAIT WAITWAITWAIT WakeTheFuckUpSamuraiWeHaveACityToBurn wallE waltuh walterShocked WalterVibe War WasZumPenis WATAFUCKEDUPDAY WatchingStream WAYTOODANK wdym WeAreLive WeDoNotCare WEDIDIT WEEWOO WeGood WePaid WHATAFUCKEDUPDAY WHAT WHATTT wheresmyhug Whenyourinnerwolfreleases WideAlERT WideCatGroove wideDvaAss WideHardo WidelebronJAM widemonkaGIGAftRobertDowneyJr wideprespeedlaugh WideRaveTime wideReacting wideSpeedLaugh3 widetime WidezyzzPls wig WineTime winton Wisdom woah Wokege WOT wot wrapitup WW wowie Xd xar2EDM xdd XDoubt xJAM xqc32 xqcBOZO xqcDespair xqcFuel xqcGoofy xqcL xqcSCHIZO xqcSlam xqcTake xqcTwerk xQcVeryWide YAAAY YamesBond YANITED YAPPING YeahThatsWhatIWouldaDid YEAHHH YEP YESS YIPIEE yonose Yoink YOOLOOKATTHISCATDOINITSLILDANCYDANCEINTOABREAKDANCEMOVE Yooo YOUDIED YouGotMe YouWouldntGetIt ZAMN ZhongXina zyzzBass zyzzJAM\"\"\"\n",
    "\n",
    "# Convert to list and clean \n",
    "emote_list = [emote.strip() for emote in tv7_emotes.split() if emote.strip()]\n",
    "\n",
    "# Remove any remaining duplicates (if any)\n",
    "unique_emotes = sorted(list(set(emote_list)))\n",
    "\n",
    "print(f\"Total unique emotes: {len(unique_emotes)}\")\n",
    "\n",
    "# Create the final shortened list\n",
    "final_emote_list = unique_emotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeecca7",
   "metadata": {},
   "source": [
    "2026 Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb57fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emotes = [\n",
    "    'ome62','Staredown','PentagramOfFarallah','Gloving','footgammaRadiation',\n",
    "    'RAAAAAAAAGH','praise','doroRage','AndreSmithing','omePIECE','Rime',\n",
    "    'doroPIECE','kaiReading','Deadge','furi','blub','duh','nuh','gopissgirl',\n",
    "    'plong','Awesome','o','feaky','DoroThinking','evol','doro18','staycalm',\n",
    "    'omeFOWL','s','fairs','deal','moshimoshi','gn'\n",
    "]\n",
    "\n",
    "final_emote_list = list(set(final_emote_list) | set(new_emotes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739a2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters in Jan 2026: 5119\n",
      "Number of Streams in Jan 2026: 28\n",
      "Number of Messages in Jan 2026: 245324\n",
      "Number of Users in Jan 2026: 11641\n",
      "\n",
      "Top 10 users in Jan 2026:\n",
      "          user  message_count\n",
      "0     erdeedge          12388\n",
      "1    rafa30___          10540\n",
      "2  polimpompis          10465\n",
      "3     JBIN2036           9478\n",
      "4     lajoss__           8257\n",
      "5     nishad13           8168\n",
      "6     HALP____           7328\n",
      "7      Muuskie           7228\n",
      "8   balintboss           5970\n",
      "9      Odah_02           4885\n",
      "\n",
      "Top 10 emotes in Jan 2026:\n",
      "hi: 9452 times\n",
      "WW: 6907 times\n",
      "LOL: 3798 times\n",
      "omeFaded: 3583 times\n",
      "ome44: 3560 times\n",
      "bye: 3328 times\n",
      "sob: 3252 times\n",
      "mhm: 3032 times\n",
      "OMEYES: 2680 times\n",
      "OOOO: 2627 times\n",
      "\n",
      "Top 5 busiest 5-minute intervals in Jan 2026:\n",
      "                          5min  message_count\n",
      "50   2026-01-03 15:35:00+01:00            618\n",
      "1409 2026-01-30 16:55:00+01:00            613\n",
      "1408 2026-01-30 16:50:00+01:00            568\n",
      "393  2026-01-10 18:30:00+01:00            530\n",
      "403  2026-01-10 19:20:00+01:00            511\n",
      "\n",
      "Stream with the highest messages per minute:\n",
      "    stream  message_count                start_time                  end_time  \\\n",
      "10     519          17635 2026-01-10 18:12:50+01:00 2026-01-10 22:47:10+01:00   \n",
      "\n",
      "    duration_min  messages_per_min  \n",
      "10    274.333333         64.283111  \n",
      "\n",
      "Chatters who chatted in every stream in Jan 2026:\n",
      "['Aluminiumminimumimmunity', 'CrystalMethod1000', 'Fossabot', 'JBIN2036', 'Nightbot', 'StreamElements', 'balintboss', 'erdeedge', 'husvetteir', 'noJokeee1', 'rafa30___']\n",
      "\n",
      "The longest stream in Jan 2026 was:\n",
      "   stream  duration_min                start_time                  end_time\n",
      "4     513         597.6 2026-01-05 15:06:47+01:00 2026-01-06 01:04:23+01:00\n",
      "\n",
      "The shortest stream in Jan 2026 was:\n",
      "   stream  duration_min                start_time                  end_time\n",
      "6     515     14.616667 2026-01-09 11:49:52+01:00 2026-01-09 12:04:29+01:00\n",
      "\n",
      "Average Daily Start Time (Earliest): 15:38\n",
      "Average Daily End Time (Latest): 20:58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Filter only January 2026\n",
    "jan_2026 = data[(data[\"date\"].dt.year == 2026) & (data[\"date\"].dt.month == 1)]\n",
    "\n",
    "# Find the first message date for each user\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message was in January 2026\n",
    "new_chatters = first_messages[\n",
    "    (first_messages[\"date\"].dt.year == 2026) & (first_messages[\"date\"].dt.month == 1)\n",
    "]\n",
    "\n",
    "# Get the number of new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "print(f\"Number of new chatters in Jan 2026: {num_new_chatters}\")\n",
    "\n",
    "stream_counts = jan_2026['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams in Jan 2026: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages in Jan 2026: {jan_2026.shape[0]}\")\n",
    "print(f\"Number of Users in Jan 2026: {jan_2026['user'].nunique()}\")\n",
    "\n",
    "# Count number of messages per user in January 2026\n",
    "user_counts = jan_2026['user'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "\n",
    "# Get the top 10 users\n",
    "top_10_users = user_counts.head(10)\n",
    "print(\"\\nTop 10 users in Jan 2026:\")\n",
    "print(top_10_users)\n",
    "\n",
    "# --- Emote Analysis ---\n",
    "emote_counter = Counter()\n",
    "\n",
    "# Go through each message and count emotes\n",
    "for message in jan_2026[\"message\"]:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "# Get top 10 emotes\n",
    "top_10_emotes = emote_counter.most_common(10)\n",
    "\n",
    "print(\"\\nTop 10 emotes in Jan 2026:\")\n",
    "for emote, count in top_10_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# --- Activity Spikes ---\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "jan_2026 = jan_2026.copy()\n",
    "\n",
    "# Round timestamps to nearest 5-minute interval\n",
    "jan_2026[\"5min\"] = jan_2026[\"date\"].dt.floor(\"5min\")\n",
    "\n",
    "# Count messages per 5-minute interval\n",
    "message_counts = jan_2026.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "# Get top 5 busiest 5-minute intervals\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 busiest 5-minute intervals in Jan 2026:\")\n",
    "print(top_5_fastest)\n",
    "\n",
    "# --- Stream Velocity ---\n",
    "# Group by stream and compute message counts and time range\n",
    "stream_stats = jan_2026.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute duration in minutes\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Avoid division by zero\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "\n",
    "# Calculate messages per minute\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Get the stream with the highest messages per minute\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "print(\"\\nStream with the highest messages per minute:\")\n",
    "print(fastest_stream)\n",
    "\n",
    "# --- Loyalty Check ---\n",
    "# Get all unique stream IDs in Jan\n",
    "all_streams = set(jan_2026[\"stream\"].unique())\n",
    "\n",
    "# Group by user and get the set of streams each user chatted in\n",
    "user_streams = jan_2026.groupby(\"user\")[\"stream\"].apply(set)\n",
    "\n",
    "# Filter users who chatted in every stream\n",
    "active_every_stream = user_streams[user_streams == all_streams]\n",
    "\n",
    "# Get just the user names\n",
    "users_in_every_stream = active_every_stream.index.tolist()\n",
    "print(\"\\nChatters who chatted in every stream in Jan 2026:\")\n",
    "print(users_in_every_stream)\n",
    "\n",
    "# Longest Stream Calculation\n",
    "longest_stream = stream_stats.sort_values(\"duration_min\", ascending=False).head(1)\n",
    "\n",
    "print(\"\\nThe longest stream in Jan 2026 was:\")\n",
    "print(longest_stream[['stream', 'duration_min', 'start_time', 'end_time']])\n",
    "\n",
    "# Shortest Stream Calculation\n",
    "shortest_stream = stream_stats.sort_values(\"duration_min\", ascending=True).head(1)\n",
    "\n",
    "print(\"\\nThe shortest stream in Jan 2026 was:\")\n",
    "print(shortest_stream[['stream', 'duration_min', 'start_time', 'end_time']])\n",
    "\n",
    "# --- Daily Average Start and End Times (Accounting for Multiple Streams) ---\n",
    "\n",
    "# 1. Create a \"logical date\" for each stream. \n",
    "# If it starts before 7 AM, it belongs to the previous calendar day's cycle.\n",
    "stream_stats[\"logical_date\"] = stream_stats[\"start_time\"].apply(\n",
    "    lambda dt: dt.date() - pd.Timedelta(days=1) if dt.hour < 7 else dt.date()\n",
    ")\n",
    "\n",
    "# Convert logical_date to a midnight datetime object so we can do math with it\n",
    "stream_stats[\"logical_date_dt\"] = pd.to_datetime(stream_stats[\"logical_date\"])\n",
    "\n",
    "# 2. Group by the logical date to find the earliest start and latest end per day\n",
    "daily_stats = stream_stats.groupby(\"logical_date\").agg(\n",
    "    daily_start=(\"start_time\", \"min\"),\n",
    "    daily_end=(\"end_time\", \"max\"),\n",
    "    logical_date_dt=(\"logical_date_dt\", \"first\") # Keep this for math\n",
    ").reset_index()\n",
    "\n",
    "# 3. Calculate how many total seconds have passed since the midnight of that logical day\n",
    "# We add .dt.tz_localize(None) to strip the timezone so pandas allows the subtraction\n",
    "start_secs_from_midnight = (\n",
    "    daily_stats[\"daily_start\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "end_secs_from_midnight = (\n",
    "    daily_stats[\"daily_end\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "# 4. Calculate the average seconds\n",
    "avg_start_sec = start_secs_from_midnight.mean()\n",
    "avg_end_sec = end_secs_from_midnight.mean()\n",
    "\n",
    "# 5. Format the seconds back into a readable HH:MM string\n",
    "def format_avg_time(seconds):\n",
    "    if pd.isna(seconds): \n",
    "        return \"N/A\"\n",
    "    # Modulo 24 hours (86400 seconds) to wrap times like 26:00 (2 AM) back to 02:00\n",
    "    normalized_secs = seconds % (24 * 3600)\n",
    "    hours = int(normalized_secs // 3600)\n",
    "    minutes = int((normalized_secs % 3600) // 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}\"\n",
    "\n",
    "print(f\"\\nAverage Daily Start Time (Earliest): {format_avg_time(avg_start_sec)}\")\n",
    "print(f\"Average Daily End Time (Latest): {format_avg_time(avg_end_sec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff83a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emotes = [\n",
    "    'ae','doroModCheck','Sup','DOSESHEKNOW','BENNED','bitrate','cortisolspike','mlem','catWait','gkeyWave','paulPls','monkeyPls','mountainlion','Walking','KILLTHATBOY','SenChest','peculiar','Senny','wideSpeedPoint','Emotional','sayWALLAHI','doroPls2','sennyTurkey','paulPiece','SCRAPETHATSHITDORO','doroConcerned','Whatever','noting','doroADHD','DRAIN','Erm','Ermm'\n",
    "]\n",
    "\n",
    "final_emote_list = list(set(final_emote_list) | set(new_emotes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e846046",
   "metadata": {},
   "source": [
    "American Arc 04-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9cff471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- American Arc Stats (2026-02-04 to 2026-02-18) ---\n",
      "Number of new chatters: 1662\n",
      "Number of Streams: 21\n",
      "Number of Messages: 105985\n",
      "Number of Users: 5744\n",
      "\n",
      "Top 10 users:\n",
      "           user  message_count\n",
      "0  FilipStayout           5794\n",
      "1       Odah_02           4741\n",
      "2   polimpompis           4551\n",
      "3      erdeedge           4418\n",
      "4     rafa30___           4248\n",
      "5      JBIN2036           3531\n",
      "6      BenXBari           3501\n",
      "7    Swimtowin1           2825\n",
      "8      nishad13           2822\n",
      "9      lajoss__           2808\n",
      "\n",
      "Top 10 emotes:\n",
      "hi: 3430 times\n",
      "WW: 2900 times\n",
      "sob: 2589 times\n",
      "LOL: 2422 times\n",
      "ome44: 1688 times\n",
      "mhm: 1579 times\n",
      "bye: 1427 times\n",
      "OOOO: 1347 times\n",
      "LMAO: 1306 times\n",
      "OMEYES: 1202 times\n",
      "\n",
      "Top 5 busiest 5-minute intervals:\n",
      "                         5min  message_count\n",
      "118 2026-02-06 19:50:00+01:00            518\n",
      "20  2026-02-05 18:15:00+01:00            505\n",
      "21  2026-02-05 18:20:00+01:00            500\n",
      "19  2026-02-05 18:10:00+01:00            444\n",
      "28  2026-02-05 18:55:00+01:00            444\n",
      "\n",
      "Stream with the highest velocity:\n",
      "   stream  messages_per_min\n",
      "1     539         63.969741\n",
      "\n",
      "The longest stream was:\n",
      "   stream  duration_min                start_time                  end_time\n",
      "4     542    407.883333 2026-02-06 17:58:20+01:00 2026-02-07 00:46:13+01:00\n",
      "\n",
      "Users who attended all 21 streams:\n",
      "['Fossabot', 'StreamElements', 'balintboss', 'gabrielzzq']\n",
      "\n",
      "Average Daily Start Time: 21:10\n",
      "Average Daily End Time: 03:05\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the American Arc window\n",
    "start_date = \"2026-02-04\"\n",
    "end_date = \"2026-02-18\"\n",
    "\n",
    "# Filter the main dataframe for the American Arc\n",
    "# Using .between() is cleaner for specific date ranges\n",
    "american_arc = data[data[\"date\"].dt.strftime('%Y-%m-%d').between(start_date, end_date)].copy()\n",
    "\n",
    "# 2. Find the first message date for each user (across the ENTIRE dataset)\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message EVER was during the American Arc\n",
    "new_chatters = first_messages[\n",
    "    first_messages[\"date\"].dt.strftime('%Y-%m-%d').between(start_date, end_date)\n",
    "]\n",
    "\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "# --- Summary Stats ---\n",
    "print(f\"--- American Arc Stats ({start_date} to {end_date}) ---\")\n",
    "print(f\"Number of new chatters: {num_new_chatters}\")\n",
    "\n",
    "stream_counts = american_arc['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages: {american_arc.shape[0]}\")\n",
    "print(f\"Number of Users: {american_arc['user'].nunique()}\")\n",
    "\n",
    "# --- Top Users ---\n",
    "user_counts = american_arc['user'].value_counts().reset_index()\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "print(\"\\nTop 10 users:\")\n",
    "print(user_counts.head(10))\n",
    "\n",
    "# --- Emote Analysis ---\n",
    "emote_counter = Counter()\n",
    "for message in american_arc[\"message\"]:\n",
    "    words = str(message).split() # Added str() to prevent errors if any non-strings exist\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "top_10_emotes = emote_counter.most_common(10)\n",
    "print(\"\\nTop 10 emotes:\")\n",
    "for emote, count in top_10_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# --- Activity Spikes ---\n",
    "american_arc[\"5min\"] = american_arc[\"date\"].dt.floor(\"5min\")\n",
    "message_counts = american_arc.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 busiest 5-minute intervals:\")\n",
    "print(top_5_fastest)\n",
    "\n",
    "# --- Stream Velocity & Duration ---\n",
    "stream_stats = american_arc.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Fastest, Longest, Shortest\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "longest_stream = stream_stats.sort_values(\"duration_min\", ascending=False).head(1)\n",
    "shortest_stream = stream_stats.sort_values(\"duration_min\", ascending=True).head(1)\n",
    "\n",
    "print(\"\\nStream with the highest velocity:\")\n",
    "print(fastest_stream[['stream', 'messages_per_min']])\n",
    "\n",
    "print(\"\\nThe longest stream was:\")\n",
    "print(longest_stream[['stream', 'duration_min', 'start_time', 'end_time']])\n",
    "\n",
    "# --- Loyalty Check ---\n",
    "all_arc_streams = set(american_arc[\"stream\"].unique())\n",
    "user_streams = american_arc.groupby(\"user\")[\"stream\"].apply(set)\n",
    "active_every_stream = user_streams[user_streams == all_arc_streams]\n",
    "\n",
    "print(f\"\\nUsers who attended all {len(all_arc_streams)} streams:\")\n",
    "print(active_every_stream.index.tolist())\n",
    "\n",
    "# --- Daily Average Times ---\n",
    "stream_stats[\"logical_date\"] = stream_stats[\"start_time\"].apply(\n",
    "    lambda dt: dt.date() - pd.Timedelta(days=1) if dt.hour < 7 else dt.date()\n",
    ")\n",
    "stream_stats[\"logical_date_dt\"] = pd.to_datetime(stream_stats[\"logical_date\"])\n",
    "\n",
    "daily_stats = stream_stats.groupby(\"logical_date\").agg(\n",
    "    daily_start=(\"start_time\", \"min\"),\n",
    "    daily_end=(\"end_time\", \"max\"),\n",
    "    logical_date_dt=(\"logical_date_dt\", \"first\")\n",
    ").reset_index()\n",
    "\n",
    "# Strip TZs for math\n",
    "start_secs = (daily_stats[\"daily_start\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]).dt.total_seconds()\n",
    "end_secs = (daily_stats[\"daily_end\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]).dt.total_seconds()\n",
    "\n",
    "print(f\"\\nAverage Daily Start Time: {format_avg_time(start_secs.mean())}\")\n",
    "print(f\"Average Daily End Time: {format_avg_time(end_secs.mean())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ea072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters during the American Arc: 1662\n",
      "\n",
      "--- Best Hour for New Chatters (American Arc) ---\n",
      "The absolute best hour was 19:00 with 242 new chatters!\n",
      "\n",
      "Full 24-Hour Breakdown:\n",
      "00:00 -> 160 new chatters\n",
      "01:00 -> 123 new chatters\n",
      "02:00 -> 87 new chatters\n",
      "03:00 -> 36 new chatters\n",
      "04:00 -> 77 new chatters\n",
      "05:00 -> 55 new chatters\n",
      "06:00 -> 26 new chatters\n",
      "07:00 -> 32 new chatters\n",
      "08:00 -> 2 new chatters\n",
      "09:00 -> 0 new chatters\n",
      "10:00 -> 0 new chatters\n",
      "11:00 -> 0 new chatters\n",
      "12:00 -> 0 new chatters\n",
      "13:00 -> 0 new chatters\n",
      "14:00 -> 0 new chatters\n",
      "15:00 -> 0 new chatters\n",
      "16:00 -> 0 new chatters\n",
      "17:00 -> 3 new chatters\n",
      "18:00 -> 183 new chatters\n",
      "19:00 -> 242 new chatters\n",
      "20:00 -> 123 new chatters\n",
      "21:00 -> 122 new chatters\n",
      "22:00 -> 206 new chatters\n",
      "23:00 -> 185 new chatters\n"
     ]
    }
   ],
   "source": [
    "# 2. Find the first message date for EVERY user in the entire dataset\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# 3. Filter to find \"New Chatters\" whose FIRST EVER message was during the American Arc\n",
    "new_chatters = first_messages[\n",
    "    first_messages[\"date\"].dt.strftime('%Y-%m-%d').between(start_date, end_date)\n",
    "].copy() # Using .copy() prevents pandas warnings later\n",
    "\n",
    "# Print total new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "print(f\"Number of new chatters during the American Arc: {num_new_chatters}\")\n",
    "\n",
    "# --- 24-Hour Breakdown for New Chatters ---\n",
    "\n",
    "# 4. Extract the hour of the day (0-23) they sent their first message\n",
    "new_chatters[\"hour\"] = new_chatters[\"date\"].dt.hour\n",
    "\n",
    "# 5. Count how many new chatters joined in each hour\n",
    "hourly_new_chatters = new_chatters.groupby(\"hour\").size().reset_index(name=\"new_chatters\")\n",
    "\n",
    "# 6. Create a 24-hour template so hours with 0 chatters still show up cleanly\n",
    "all_hours = pd.DataFrame({\"hour\": range(24)})\n",
    "hourly_stats = all_hours.merge(hourly_new_chatters, on=\"hour\", how=\"left\").fillna(0)\n",
    "\n",
    "# Convert the count to an integer (merge/fillna sometimes turns them into decimals)\n",
    "hourly_stats[\"new_chatters\"] = hourly_stats[\"new_chatters\"].astype(int)\n",
    "\n",
    "# 7. Find the absolute best hour\n",
    "best_hour_row = hourly_stats.sort_values(\"new_chatters\", ascending=False).iloc[0]\n",
    "best_hour = int(best_hour_row['hour'])\n",
    "best_count = int(best_hour_row['new_chatters'])\n",
    "\n",
    "# --- Print the Results ---\n",
    "print(\"\\n--- Best Hour for New Chatters (American Arc) ---\")\n",
    "print(f\"The absolute best hour was {best_hour:02d}:00 with {best_count} new chatters!\\n\")\n",
    "\n",
    "print(\"Full 24-Hour Breakdown:\")\n",
    "for index, row in hourly_stats.iterrows():\n",
    "    hour_str = f\"{int(row['hour']):02d}:00\"\n",
    "    print(f\"{hour_str} -> {int(row['new_chatters'])} new chatters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cffdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters in Feb 2026: 2637\n",
      "Number of Streams in Feb 2026: 30\n",
      "Number of Messages in Feb 2026: 171755\n",
      "Number of Users in Feb 2026: 7950\n",
      "\n",
      "Top 10 users in Feb 2026:\n",
      "           user  message_count\n",
      "0  FilipStayout           8882\n",
      "1     rafa30___           8238\n",
      "2      nishad13           7627\n",
      "3      erdeedge           7571\n",
      "4   polimpompis           6939\n",
      "5       Odah_02           6423\n",
      "6      JBIN2036           5999\n",
      "7    Swimtowin1           4550\n",
      "8    balintboss           4189\n",
      "9      BenXBari           3872\n",
      "\n",
      "Top 10 emotes in Feb 2026:\n",
      "hi: 5404 times\n",
      "sob: 4168 times\n",
      "WW: 4060 times\n",
      "LOL: 3689 times\n",
      "mhm: 2536 times\n",
      "LMAO: 2331 times\n",
      "bye: 2205 times\n",
      "OMEYES: 2160 times\n",
      "OOOO: 2136 times\n",
      "ome44: 1970 times\n",
      "\n",
      "Top 5 busiest 5-minute intervals in Feb 2026:\n",
      "                         5min  message_count\n",
      "185 2026-02-06 19:50:00+01:00            518\n",
      "87  2026-02-05 18:15:00+01:00            505\n",
      "88  2026-02-05 18:20:00+01:00            500\n",
      "86  2026-02-05 18:10:00+01:00            444\n",
      "95  2026-02-05 18:55:00+01:00            444\n",
      "\n",
      "Stream with the highest messages per minute:\n",
      "   stream  message_count                start_time                  end_time  \\\n",
      "3     539           8879 2026-02-05 18:04:33+01:00 2026-02-05 20:23:21+01:00   \n",
      "\n",
      "   duration_min  messages_per_min  \n",
      "3         138.8         63.969741  \n",
      "\n",
      "Chatters who chatted in every stream in Feb 2026:\n",
      "['Fossabot', 'StreamElements', 'balintboss']\n",
      "\n",
      "The longest stream in Feb 2026 was:\n",
      "    stream  duration_min                start_time                  end_time\n",
      "28     564    408.083333 2026-02-27 15:45:49+01:00 2026-02-27 22:33:54+01:00\n",
      "\n",
      "The shortest stream in Feb 2026 was:\n",
      "    stream  duration_min                start_time                  end_time\n",
      "22     558          51.7 2026-02-17 18:47:47+01:00 2026-02-17 19:39:29+01:00\n",
      "\n",
      "Average Daily Start Time (Earliest): 19:21\n",
      "Average Daily End Time (Latest): 00:37\n"
     ]
    }
   ],
   "source": [
    "# Filter only February 2026\n",
    "feb_2026 = data[(data[\"date\"].dt.year == 2026) & (data[\"date\"].dt.month == 2)]\n",
    "\n",
    "# Find the first message date for each user\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message was in February 2026\n",
    "new_chatters = first_messages[\n",
    "    (first_messages[\"date\"].dt.year == 2026) & (first_messages[\"date\"].dt.month == 2)\n",
    "]\n",
    "\n",
    "# Get the number of new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "print(f\"Number of new chatters in Feb 2026: {num_new_chatters}\")\n",
    "\n",
    "stream_counts = feb_2026['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams in Feb 2026: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages in Feb 2026: {feb_2026.shape[0]}\")\n",
    "print(f\"Number of Users in Feb 2026: {feb_2026['user'].nunique()}\")\n",
    "\n",
    "# Count number of messages per user in February 2026\n",
    "user_counts = feb_2026['user'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "\n",
    "# Get the top 10 users\n",
    "top_10_users = user_counts.head(10)\n",
    "print(\"\\nTop 10 users in Feb 2026:\")\n",
    "print(top_10_users)\n",
    "\n",
    "# --- Emote Analysis ---\n",
    "emote_counter = Counter()\n",
    "\n",
    "# Go through each message and count emotes\n",
    "for message in feb_2026[\"message\"]:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "# Get top 10 emotes\n",
    "top_10_emotes = emote_counter.most_common(10)\n",
    "\n",
    "print(\"\\nTop 10 emotes in Feb 2026:\")\n",
    "for emote, count in top_10_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# --- Activity Spikes ---\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "feb_2026 = feb_2026.copy()\n",
    "\n",
    "# Round timestamps to nearest 5-minute interval\n",
    "feb_2026[\"5min\"] = feb_2026[\"date\"].dt.floor(\"5min\")\n",
    "\n",
    "# Count messages per 5-minute interval\n",
    "message_counts = feb_2026.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "# Get top 5 busiest 5-minute intervals\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 busiest 5-minute intervals in Feb 2026:\")\n",
    "print(top_5_fastest)\n",
    "\n",
    "# --- Stream Velocity ---\n",
    "# Group by stream and compute message counts and time range\n",
    "stream_stats = feb_2026.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute duration in minutes\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Avoid division by zero\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "\n",
    "# Calculate messages per minute\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Get the stream with the highest messages per minute\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "print(\"\\nStream with the highest messages per minute:\")\n",
    "print(fastest_stream)\n",
    "\n",
    "# --- Loyalty Check ---\n",
    "# Get all unique stream IDs in Feb\n",
    "all_streams = set(feb_2026[\"stream\"].unique())\n",
    "\n",
    "# Group by user and get the set of streams each user chatted in\n",
    "user_streams = feb_2026.groupby(\"user\")[\"stream\"].apply(set)\n",
    "\n",
    "# Filter users who chatted in every stream\n",
    "active_every_stream = user_streams[user_streams == all_streams]\n",
    "\n",
    "# Get just the user names\n",
    "users_in_every_stream = active_every_stream.index.tolist()\n",
    "print(\"\\nChatters who chatted in every stream in Feb 2026:\")\n",
    "print(users_in_every_stream)\n",
    "\n",
    "#Longest Stream Calculation\n",
    "longest_stream = stream_stats.sort_values(\"duration_min\", ascending=False).head(1)\n",
    "\n",
    "print(\"\\nThe longest stream in Feb 2026 was:\")\n",
    "print(longest_stream[['stream', 'duration_min', 'start_time', 'end_time']])\n",
    "\n",
    "#Shortest Stream Calculation\n",
    "shortest_stream = stream_stats.sort_values(\"duration_min\", ascending=True).head(1)\n",
    "\n",
    "print(\"\\nThe shortest stream in Feb 2026 was:\")\n",
    "print(shortest_stream[['stream', 'duration_min', 'start_time', 'end_time']])\n",
    "\n",
    "# --- Daily Average Start and End Times (Accounting for Multiple Streams) ---\n",
    "\n",
    "# 1. Create a \"logical date\" for each stream. \n",
    "# If it starts before 7 AM, it belongs to the previous calendar day's cycle.\n",
    "stream_stats[\"logical_date\"] = stream_stats[\"start_time\"].apply(\n",
    "    lambda dt: dt.date() - pd.Timedelta(days=1) if dt.hour < 7 else dt.date()\n",
    ")\n",
    "\n",
    "# Convert logical_date to a midnight datetime object so we can do math with it\n",
    "stream_stats[\"logical_date_dt\"] = pd.to_datetime(stream_stats[\"logical_date\"])\n",
    "\n",
    "# 2. Group by the logical date to find the earliest start and latest end per day\n",
    "daily_stats = stream_stats.groupby(\"logical_date\").agg(\n",
    "    daily_start=(\"start_time\", \"min\"),\n",
    "    daily_end=(\"end_time\", \"max\"),\n",
    "    logical_date_dt=(\"logical_date_dt\", \"first\") # Keep this for math\n",
    ").reset_index()\n",
    "\n",
    "# 3. Calculate how many total seconds have passed since the midnight of that logical day\n",
    "# We add .dt.tz_localize(None) to strip the timezone so pandas allows the subtraction\n",
    "start_secs_from_midnight = (\n",
    "    daily_stats[\"daily_start\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "end_secs_from_midnight = (\n",
    "    daily_stats[\"daily_end\"].dt.tz_localize(None) - daily_stats[\"logical_date_dt\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "# 4. Calculate the average seconds\n",
    "avg_start_sec = start_secs_from_midnight.mean()\n",
    "avg_end_sec = end_secs_from_midnight.mean()\n",
    "\n",
    "# 5. Format the seconds back into a readable HH:MM string\n",
    "def format_avg_time(seconds):\n",
    "    if pd.isna(seconds): \n",
    "        return \"N/A\"\n",
    "    # Modulo 24 hours (86400 seconds) to wrap times like 26:00 (2 AM) back to 02:00\n",
    "    normalized_secs = seconds % (24 * 3600)\n",
    "    hours = int(normalized_secs // 3600)\n",
    "    minutes = int((normalized_secs % 3600) // 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}\"\n",
    "\n",
    "print(f\"\\nAverage Daily Start Time (Earliest): {format_avg_time(avg_start_sec)}\")\n",
    "print(f\"Average Daily End Time (Latest): {format_avg_time(avg_end_sec)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
