{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a61989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pytz\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58865add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of filenames from the configuration file\n",
    "with open('../file_list.txt', 'r', encoding='utf-8') as config_file:\n",
    "    file_names = config_file.read().splitlines()\n",
    "\n",
    "# Regex pattern to match the data format\n",
    "pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "\n",
    "# Initialize an empty list to store parsed data\n",
    "datalist = []\n",
    "stream_count = 0\n",
    "# Iterate over each specified file\n",
    "for file in file_names:\n",
    "    full_path = f\"../data/{file}\"\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, user, message = match.groups()\n",
    "                datalist.append([date, user, message,stream_count])\n",
    "    stream_count = stream_count + 1\n",
    "\n",
    "# Create a DataFrame from the parsed data\n",
    "data = pd.DataFrame(datalist, columns=[\"date\", \"user\", \"message\",\"stream\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19eaac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55691696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_utc_to_cet(df, date_column='date'):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamps to Central European Time (CET/CEST) with proper DST handling\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the date column\n",
    "    date_column (str): Name of the column containing UTC timestamps\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with converted timestamps\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure timestamps are UTC aware\n",
    "    if df[date_column].dt.tz is None:\n",
    "        df[date_column] = df[date_column].dt.tz_localize('UTC')\n",
    "    elif df[date_column].dt.tz != pytz.UTC:\n",
    "        df[date_column] = df[date_column].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Convert to CET/CEST (Europe/Berlin includes proper DST handling)\n",
    "    df[date_column] = df[date_column].dt.tz_convert('Europe/Berlin')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5700f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_utc_to_cet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3b966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data[\"user\"] = data[\"user\"].replace(\"Banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_conbacon\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Wirelesss_\", \"W1r3lesss\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"treklul\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"ttrek_\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"TriplesingleJ\", \"TripleSingleJames\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uuccugr\", \"uwu_cougar\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uuccugr\", \"uuccugr_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ae4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all unique usernames\n",
    "unique_users = data['user'].unique()\n",
    "\n",
    "# Create a mapping from lowercase username to all variants\n",
    "\n",
    "user_variants = defaultdict(set)\n",
    "for user in unique_users:\n",
    "    user_variants[user.lower()].add(user)\n",
    "\n",
    "# Find usernames with different capitalization\n",
    "duplicate_users = {k: v for k, v in user_variants.items() if len(v) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32df7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from all variants to the canonical (sorted first) variant\n",
    "variant_map = {}\n",
    "for variants in duplicate_users.values():\n",
    "    sorted_variants = sorted(variants)\n",
    "    canonical = sorted_variants[0]\n",
    "    for v in variants:\n",
    "        variant_map[v] = canonical\n",
    "\n",
    "# Replace usernames in 'user' column\n",
    "data['user'] = data['user'].apply(lambda u: variant_map.get(u, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae0e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique emotes: 935\n"
     ]
    }
   ],
   "source": [
    "# Original combined lists with duplicates removed and cleaned\n",
    "tv7_emotes = \"\"\"shomonting thinking agafat FirstTimeClanka DORVIS gasp RISING dd uncPLS domgBruh Deafge OMEGADANCEBUTFASTER kim3 Buggin speed8 .... d32 ome54 ohok minionBike Clown Explosion hackingCD JermaSoy MathTime MoneyRain PokiShare TakingNotes :0 :3 :33 :tf: !boost +1 0pixel 1DLove 3Head 3Heading 4House 4Shrug 4Weird 5Head AAAA Acknowledged ACTINUP ADHD agahi AIM AIRBALL AIWITHTHEBRAIDS Alarm ALE Alfred AlienDance AlienPls AlienPls2 AlienPls3 Aloo Alright amongE ANGRE ANOTHERONE Ant AREYOUAGIRL AREYOUAGIRLFtxQcYellingAtYou areyoufr AreYouSeriousRightNeow arnoldHalt arthur Assept AURA AwHellNah Aware AWOO AWOOGA axelF ayo bah BAND Banger BANGER banties Barack barryArrive Barry63 BantiesPaulBeef Based BASED BatChestAbove batman Batman batJAM batPls Beatles Bedge BEG BEGGING Bello BigD bieberDougie BELIEVERS BibleThump BINGO Bleh Bloons BOOBA bog BOOM BOOMIES BOINK BORGIR Borfday brbToilet Broadcaster brb Bruh BRUHMM bruv buh buhbye buhFlipExplode BUSSIN BUSSERS bye CanIHaveADollar cannySilly catAsk catBusiness CatCozy catDespair catEat catJAM catKiss catPls catSigh catSmash CatTime catTwerk CAUGHT Caught CaughtIn4K Celebrating CHADDING characterSelected CHATTERS chatting cheerleaders ChillGuy Chillin Cinema chilling clappi Clap classic CLEAN Clueless CLIPPERS CLOWNDETECTED COCKA cokeBreak COMEHERE Concerned Considering Cooked COPIUM crabPls Crunch CS2 Cuck Cuh D: damily Damn dansi dash Dave deadassFaint Delusional DemonTiming Dentge despair DespairRyan Devious DIESOFCRINGE Dime DinkDonk Dinema doggoSlava dogJAM DogLookingWickedAndCool doid dojaPls dome44 dome32 donowall donoWall doroAunt doroBleh dorobubu doroCD DoroCheer doroFiddy doroFlex doroGHOST doroHEAD doroKick doroL doroMAD doroPray doroRip doroSoy DoroTalkingAgain Dorozea doster DOUBTERS DRAIN Drake DRAMA dreamwastaken drooling drukiDnace drukiDnace2 duaKiss dudWhat EDGE EDM EDITING emo erinNya essaying ewphop eww EZ EZdodge Exerpas Explosion eyeroll fadedthanaho FARMING FeelsBadMan FeelsDankMan FeelsBlackScreen FeelsGladMan FeelsLagMan FeelsLateMan FeelsOkayMan FeelsStrongMan FeelsTiredMan FeelsWeirdMan FeelsWowMan fein FEINFEINFEINFEINFEINFEINFEINFEI FellOff fembajJAM Fiddy FiddyWtf FINALLY firewriting FirstTime FirstTimeBackseating FirstTimeChadder FirstTimeChatter FirstTimeEmoteFail FirstTimeGooner FirstTimePepega FirstTimeTest firsttimebuh FLASHBANG flightnotL Flirt Flushed fnaf footstep forsenCD forsenLaughingAtYou ForsenSingingAtYou forsenPls fortnite fr freakbob freakyfredday freddy Freedom FUNNY g32 GAGAGA gachiGASM gachiHYPER gamily GAMBA GameplayTime GAMING GatieG Gaught GENIUS GetALoadOfThisGuy gg GIGACHAD GIGACHAIR GIGACLAUS GIGAMODS GIGAMOD gigl gkeyFlip gkeyPregnantBounce gkeySMP gkeyUwu gkeywide gkeyWiding gkitten GivenUp girlBoss gkitten glorpaga glorpdetective glorp GlorpMeeting glorprave gmoney goaler goat goblin44 GODDID Gogging GoodBye Gooner gooner GoodTake GOONING GotCaughtTrolling GotEEM gothKiss gPls greetingsladies GREEDY GROOTING GRRR GULP GuitarTime GYAT HABIBI hackingCD HACKERMANS hai HAH HaltEinfachDeineFresseDuHurensohn HandsUp happi HARAM HarryStylesKiss Headbang healed HECOOKING heh HEHE HEHEHEHA Heisenberj HELLO HELP helvete Herewego hesRight heyywithrizz HEYYY hi hiii hiiii Hmm HOBBY HOLY HolyFuck homelessPOV HowDoWeTellHer HowDoWeTellHim hue HUH HUHHHHHHHHHH iAsked ICANT idiot iDrive IFISPEAK IfYouCantSeeThisEmoteUseExclamationMark7tv Ignored IGON imback IMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGET ImNotOk ImtiredBoss INTENSEGAMING islandboy ISeeYou itsover itstime Jackass jacob1 jacob2 jacob3 jacob4 jah Jammies JARVIS Jay JermaSoy jiggy job JOB Joel joever john Johnporkiscalling JokerHAHA JokerLaugh juh JumpScared JUMPSCARE JustAChillGuy JustAnotherDay JustHowItIs justinbieber KaiCenatOhiogyatwithskibiditoiletwatchingtheWrizzhappeningrightinfrontofhimwithfanumtaxtaxingthegyat KanyeStare KEKW KENOUGH KeyShaker kim3 kittyBANGER kittyBop KKalinka KKonaW KKool kratos Lamonting LastTimeChatter lava lebronArrive lebronJAM lebronTROLL LEBRONNN lemon Lemon LetsBingo LETHERCOOK LETSFUCKINGJOE LETSGO LieMeter life Life Listening LiterallyMe Lithuanian LittleTrolling LiveReaction LL LMAOFREAKY lmao Loading LOCKIN lockedin LOL Looking LookUp lore luh lurkk luton LULE LULW MAJ Madge ManchesterUnited Massive? MarblesTime Martin matSad maxwin MeRN me: MeWhenIBuyEgyptianProperty MEGALUL mee Memories merch mhm MicTime mikuPls mindloud modCheck ModAbuse MODDING Modding mods MODS Mog monakS monday monkeyListening monkeySip MONKA monkaTOS monkaW MONKE MinionHoting MoneyRain muted mutted MUGA MVP MVPOfFarallah MYHEARTILOVEDHER myIQ MYLIFE NAILS NAILSING NAHH NAHHH NAHHHH nananAYAYA NAUR NAvsEU Nerd niceguy NOCHECKMARKS NODDERS NOIDONTTHINKSO NoMaidens NOOPERS NOOOOO NOHORNY noonecares NOSHOT NOTED notListening notxqcL NOW NOWAY NOWAYING np nt nuhuh nyehehehe nyanPls nya o7 Ogre ohhh ohhhhhhhhh OHMYGAWDD ohneFinger ohno ohSHIT oj OK Okei okak OLDWORK OM om omE ome10 ome101 ome104 ome105 ome14 ome15 ome18 ome21 ome29 ome32 ome4 ome41 ome44 ome44444444 ome47 ome5 ome51 ome52 ome55 ome57 ome67 ome69 ome79 ome808 ome83 ome9 ome90 ome96 ome99 OMEGADANCE OMEGALUL OMEGALULiguess omEE omeJAM omeJudging omeOhSHIT omeScrajj omeStare OMEYES omeWiggle OMFG omgBruh ongang OneGuy ONEMORE OnMyWayToDoroMomHouse OOOO oopsie otag OuttaPocket OVERWATCH OVERWORKING OverwhelminglyWholesome owoCheer PagBounce PagChomp PagMan Panam parasocial Parasocial PARASOCIAL paris paul Paul paulNya PauseMan PAUSENEMOGU Peace PEEPEES peepoAds peepoBox peepoBelievers peepoClap peepoComfy peepoDJ peepoDoubters peepoEvil peepoFarmer peepoFat peepoGiggles peepoHappy peepoHey peepoHug peepoKiss peepoLeave peepoLegs peepoLove peepoMarch peepoPls peepoPride peepoRiot peepoSad peepoShy peepoSmile peepoStop peepoTalk pepeAgony pepeGun PepeHands pepeJAM PepeLaugh pepePoint PepePls pepeW Pepega PepegaAim PepegaChat PepegaReading PepoG Petter Pffttt Pffttt2 phew phpk pickle PianoTime Pipege pKitten pL Please pleading plink-182 plinkVibe plonk pmo Plotge PogO PogU pointless pokiFlirt pol POLICE Pondering popipopipipopipo poroPls POVbornbefore2000 ppHop ppL ppOverheat Prayge prePffttt PRIMERS PTSD pulNya PuzzleTime qq ragebait RAGEY RAHH RainTime RAMBOLMG RareParrot ratomilton RaveDance RaveTime ratio Reacting RealForsen ReallyMad RebeccaBlack Reddit RememberTheDays RibertJam RiddleMeThis RIPBOZO RIRI Rizzler RobertJam ROFL RoxyPotato RUNNING rt ryanArrive Sadding Sadge SADge SAJ SAVEME SCATTER saythatagain scawy SCHEISSE SCHIZO SCRAPETHATSHITJOHNNY SCHTOP sdd SERIOUSLY SEXO shogaNya Shits shutup Shruge silliness sisyphus Sippin Sits skip SLAY Sleepo Smile smh Smoge SmurfHey Smurfing SNACKING SNEAK SNIFFA sob SOLARFLARE songbird sotruebestie SOYSCREAM Speechless speed1 speed2 speed21 speed25 speed32 speed4 speed44 speed8 speed88 speedVibe spfLEAN:()wiltee_()tonyhawkproskater4:-:-:_FREEWAVE3-encinoman--:enteringwalmart:-wheezethelean-123 SpeedLaugh SpeedLeft SpeedR spongePls squadHips Stare Staring steve Steve SteerR StreamEnding STREAMER STREAMERSGIVINGTHEWORSTFUCKINGTAKESINEXISTENCE StoryTime Surfing SurE sus susDog Susge SUSSY Swag swagJAM ta tak TakingNotes TeamEDWARD test THATHIT ThatsJustMe ThePaulers TheVoices TheWolfInMe Thinking Thinking2 ThisChat ThisIsMinecraft TIMEOUT Tomfoolery totallylistening TriJam TriKool TriSad TRUEING TRIVSsorry ts Tuckge tuff TWEAK typeshit typhu UGH um UltraMad unibrow unemployment unmod uwu uuh VALORANT veryDoro VeryKey VeryPog VeryPogftxQcInTheShower vibePls VibePls VIDEOGAME VIEWERS vips Voices wade Waddup waga wah waiting Waiting WAHHH WAJAJA WAIT WAITWAITWAIT WakeTheFuckUpSamuraiWeHaveACityToBurn wallE waltuh walterShocked WalterVibe War WasZumPenis WATAFUCKEDUPDAY WatchingStream WAYTOODANK wdym WeAreLive WeDoNotCare WEDIDIT WEEWOO WeGood WePaid WHATAFUCKEDUPDAY WHAT WHATTT wheresmyhug Whenyourinnerwolfreleases WideAlERT WideCatGroove wideDvaAss WideHardo WidelebronJAM widemonkaGIGAftRobertDowneyJr wideprespeedlaugh WideRaveTime wideReacting wideSpeedLaugh3 widetime WidezyzzPls wig WineTime winton Wisdom woah Wokege WOT wot wrapitup WW wowie Xd xar2EDM xdd XDoubt xJAM xqc32 xqcBOZO xqcDespair xqcFuel xqcGoofy xqcL xqcSCHIZO xqcSlam xqcTake xqcTwerk xQcVeryWide YAAAY YamesBond YANITED YAPPING YeahThatsWhatIWouldaDid YEAHHH YEP YESS YIPIEE yonose Yoink YOOLOOKATTHISCATDOINITSLILDANCYDANCEINTOABREAKDANCEMOVE Yooo YOUDIED YouGotMe YouWouldntGetIt ZAMN ZhongXina zyzzBass zyzzJAM\"\"\"\n",
    "\n",
    "# Convert to list and clean up\n",
    "emote_list = [emote.strip() for emote in tv7_emotes.split() if emote.strip()]\n",
    "\n",
    "# Remove any remaining duplicates (though the manual cleanup should have gotten most)\n",
    "unique_emotes = sorted(list(set(emote_list)))\n",
    "\n",
    "print(f\"Total unique emotes: {len(unique_emotes)}\")\n",
    "\n",
    "# Create the final shortened list\n",
    "final_emote_list = unique_emotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a213a",
   "metadata": {},
   "source": [
    "july_2025:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf1ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters in July 2025: 4625\n",
      "Number of Streams in July 2025: 26\n",
      "Number of Messages in July 2025: 179825\n",
      "Number of Users in July 2025: 9840\n",
      "          user  message_count\n",
      "0     BenXBari           8624\n",
      "1     JBIN2036           7848\n",
      "2      trek44_           6030\n",
      "3  Georgie1471           5324\n",
      "4   cr7vaibhav           5013\n",
      "5   balintboss           4639\n",
      "6    W1r3lesss           4484\n",
      "7     rautsi__           3970\n",
      "8    SchiKen44           2976\n",
      "9      Typhu25           2513\n",
      "Top 5 emotes in July 2025:\n",
      "LOL: 4934 times\n",
      "hai: 2794 times\n",
      "OMEYES: 2746 times\n",
      "OOOO: 2502 times\n",
      "WW: 2063 times\n",
      "Top 3 words in July 2025:\n",
      "the: 12611 times\n",
      "you: 10652 times\n",
      "i: 8784 times\n",
      "Top 5 busiest 5-minute intervals in July 2025:\n",
      "                          5min  message_count\n",
      "44   2025-07-03 18:05:00+02:00            729\n",
      "45   2025-07-03 18:10:00+02:00            609\n",
      "38   2025-07-03 17:35:00+02:00            602\n",
      "1376 2025-07-31 15:30:00+02:00            567\n",
      "37   2025-07-03 17:30:00+02:00            551\n",
      "Stream with the highest messages per minute:\n",
      "    stream  message_count                start_time                  end_time  \\\n",
      "25     381          11122 2025-07-31 14:47:04+02:00 2025-07-31 18:18:35+02:00   \n",
      "\n",
      "    duration_min  messages_per_min  \n",
      "25    211.516667         52.582145  \n",
      "chatters who chatted in every stream in July 2025:\n",
      "['Aluminiumminimumimmunity', 'ConorNewe', 'DonMascarpon', 'Fossabot', 'Georgie1471', 'JBIN2036', 'PurpCodd', 'StreamElements', 'W1r3lesss', 'balintboss', 'cr7vaibhav', 'trek44_']\n"
     ]
    }
   ],
   "source": [
    "# Filter only July 2025\n",
    "july_2025 = data[(data[\"date\"].dt.year == 2025) & (data[\"date\"].dt.month == 7)]\n",
    "\n",
    "# Find the first message date for each user\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message was in July 2025\n",
    "new_chatters = first_messages[\n",
    "    (first_messages[\"date\"].dt.year == 2025) & (first_messages[\"date\"].dt.month == 7)\n",
    "]\n",
    "\n",
    "# Get the number of new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "print(f\"Number of new chatters in July 2025: {num_new_chatters}\")\n",
    "stream_counts = july_2025['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams in July 2025: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages in July 2025: {july_2025.shape[0]}\")\n",
    "print(f\"Number of Users in July 2025: {july_2025['user'].nunique()}\")\n",
    "user_counts = july_2025['user'].value_counts().reset_index()\n",
    "user_counts.sort_values('count').tail(10)\n",
    "\n",
    "# Count number of messages per user in July 2025\n",
    "user_counts = july_2025['user'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "\n",
    "# Get the top 3 users\n",
    "top_10_users = user_counts.head(10)\n",
    "\n",
    "print(top_10_users)\n",
    "\n",
    "# Initialize a Counter to store emote frequencies\n",
    "emote_counter = Counter()\n",
    "\n",
    "# Go through each message and count emotes\n",
    "for message in july_2025[\"message\"]:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "# Get top 3 emotes\n",
    "top_5_emotes = emote_counter.most_common(5)\n",
    "\n",
    "print(\"Top 5 emotes in July 2025:\")\n",
    "for emote, count in top_5_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Count all words from July messages\n",
    "word_counter = Counter()\n",
    "\n",
    "for message in july_2025[\"message\"]:\n",
    "    words = message.lower().split()\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Get top 3 most common words\n",
    "top_3_words = word_counter.most_common(3)\n",
    "\n",
    "print(\"Top 3 words in July 2025:\")\n",
    "for emote, count in top_3_words:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "july_2025 = july_2025.copy()\n",
    "\n",
    "# Round timestamps to nearest 5-minute interval\n",
    "july_2025[\"5min\"] = july_2025[\"date\"].dt.floor(\"5min\")\n",
    "\n",
    "# Count messages per 5-minute interval\n",
    "message_counts = july_2025.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "# Get top 5 busiest 5-minute intervals\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"Top 5 busiest 5-minute intervals in July 2025:\")\n",
    "print(top_5_fastest)\n",
    "# Group by stream and compute message counts and time range\n",
    "stream_stats = july_2025.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute duration in minutes\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Avoid division by zero\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "\n",
    "# Calculate messages per minute\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Get the stream with the highest messages per minute\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "print(\"Stream with the highest messages per minute:\")\n",
    "print(fastest_stream)\n",
    "\n",
    "# Get all unique stream IDs in July\n",
    "all_streams = set(july_2025[\"stream\"].unique())\n",
    "# Group by user and get the set of streams each user chatted in\n",
    "user_streams = july_2025.groupby(\"user\")[\"stream\"].apply(set)\n",
    "\n",
    "# Filter users who chatted in every stream\n",
    "active_every_stream = user_streams[user_streams == all_streams]\n",
    "\n",
    "# Get just the user names\n",
    "users_in_every_stream = active_every_stream.index.tolist()\n",
    "print(\"chatters who chatted in every stream in July 2025:\")\n",
    "print(users_in_every_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e88a537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters in May 2024: 2266\n",
      "Number of Streams in May 2024: 29\n",
      "Number of Messages in May 2024: 83053\n",
      "Number of Users in May 2024: 2266\n",
      "             user  message_count\n",
      "0    Martin_Gales           7506\n",
      "1       banties_x           3670\n",
      "2            oJov           3302\n",
      "3       1206paul_           2506\n",
      "4  StreamElements           2403\n",
      "5           roxa0           2309\n",
      "6           LX212           2226\n",
      "7     IvanOnMyOwn           2089\n",
      "8         klimzaa           1990\n",
      "9         Risc__V           1527\n",
      "Top 5 emotes in May 2024:\n",
      "mhm: 1967 times\n",
      "omE: 1368 times\n",
      "o7: 625 times\n",
      "OMEGALUL: 502 times\n",
      "hi: 474 times\n",
      "Top 3 words in May 2024:\n",
      "the: 11231 times\n",
      "you: 7300 times\n",
      "i: 7155 times\n",
      "Top 5 busiest 5-minute intervals in May 2024:\n",
      "                          5min  message_count\n",
      "1494 2024-05-27 23:55:00+02:00            339\n",
      "1149 2024-05-22 15:45:00+02:00            292\n",
      "1495 2024-05-28 00:00:00+02:00            225\n",
      "1558 2024-05-28 21:55:00+02:00            186\n",
      "1643 2024-05-29 19:55:00+02:00            172\n",
      "Stream with the highest messages per minute:\n",
      "    stream  message_count                start_time                  end_time  \\\n",
      "19      19           2089 2024-05-22 14:59:18+02:00 2024-05-22 17:03:08+02:00   \n",
      "\n",
      "    duration_min  messages_per_min  \n",
      "19    123.833333         16.869448  \n",
      "Chatters who chatted in every stream in May 2024:\n",
      "['1206paul_', 'Martin_Gales', 'StreamElements', 'balintboss', 'banties_x', 'dorozea']\n"
     ]
    }
   ],
   "source": [
    "# Filter only May 2024\n",
    "may_2024 = data[(data[\"date\"].dt.year == 2024) & (data[\"date\"].dt.month == 5)]\n",
    "\n",
    "# Find the first message date for each user\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message was in May 2024\n",
    "new_chatters = first_messages[\n",
    "    (first_messages[\"date\"].dt.year == 2024) & (first_messages[\"date\"].dt.month == 5)\n",
    "]\n",
    "\n",
    "# Get the number of new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "print(f\"Number of new chatters in May 2024: {num_new_chatters}\")\n",
    "stream_counts = may_2024['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams in May 2024: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages in May 2024: {may_2024.shape[0]}\")\n",
    "print(f\"Number of Users in May 2024: {may_2024['user'].nunique()}\")\n",
    "user_counts = may_2024['user'].value_counts().reset_index()\n",
    "user_counts.sort_values('count').tail(10)\n",
    "\n",
    "# Count number of messages per user in May 2024\n",
    "user_counts = may_2024['user'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "\n",
    "# Get the top 10 users\n",
    "top_10_users = user_counts.head(10)\n",
    "\n",
    "print(top_10_users)\n",
    "\n",
    "# Initialize a Counter to store emote frequencies\n",
    "emote_counter = Counter()\n",
    "\n",
    "# Go through each message and count emotes\n",
    "for message in may_2024[\"message\"]:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "# Get top 5 emotes\n",
    "top_5_emotes = emote_counter.most_common(5)\n",
    "\n",
    "print(\"Top 5 emotes in May 2024:\")\n",
    "for emote, count in top_5_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Count all words from May messages\n",
    "word_counter = Counter()\n",
    "\n",
    "for message in may_2024[\"message\"]:\n",
    "    words = message.lower().split()\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Get top 3 most common words\n",
    "top_3_words = word_counter.most_common(3)\n",
    "\n",
    "print(\"Top 3 words in May 2024:\")\n",
    "for emote, count in top_3_words:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "may_2024 = may_2024.copy()\n",
    "\n",
    "# Round timestamps to nearest 5-minute interval\n",
    "may_2024[\"5min\"] = may_2024[\"date\"].dt.floor(\"5min\")\n",
    "\n",
    "# Count messages per 5-minute interval\n",
    "message_counts = may_2024.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "# Get top 5 busiest 5-minute intervals\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"Top 5 busiest 5-minute intervals in May 2024:\")\n",
    "print(top_5_fastest)\n",
    "\n",
    "# Group by stream and compute message counts and time range\n",
    "stream_stats = may_2024.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute duration in minutes\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Avoid division by zero\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "\n",
    "# Calculate messages per minute\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Get the stream with the highest messages per minute\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "print(\"Stream with the highest messages per minute:\")\n",
    "print(fastest_stream)\n",
    "\n",
    "# Get all unique stream IDs in May\n",
    "all_streams = set(may_2024[\"stream\"].unique())\n",
    "\n",
    "# Group by user and get the set of streams each user chatted in\n",
    "user_streams = may_2024.groupby(\"user\")[\"stream\"].apply(set)\n",
    "\n",
    "# Filter users who chatted in every stream\n",
    "active_every_stream = user_streams[user_streams == all_streams]\n",
    "\n",
    "# Get just the user names\n",
    "users_in_every_stream = active_every_stream.index.tolist()\n",
    "print(\"Chatters who chatted in every stream in May 2024:\")\n",
    "print(users_in_every_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d910a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new chatters in May 2024: 179\n",
      "Number of Streams in May 2024: 1\n",
      "Number of Messages in May 2024: 8622\n",
      "Number of Users in May 2024: 767\n",
      "              user  message_count\n",
      "0         BenXBari            757\n",
      "1         rautsi__            427\n",
      "2         JBIN2036            386\n",
      "3       cr7vaibhav            382\n",
      "4         HALP____            319\n",
      "5       balintboss            254\n",
      "6          Odah_02            234\n",
      "7  nishad_more1311            213\n",
      "8        rafa30___            170\n",
      "9         KrieSeaX            165\n",
      "Top 5 emotes in May 2024:\n",
      "sob: 191 times\n",
      "thinking: 186 times\n",
      "LOL: 184 times\n",
      "WW: 165 times\n",
      "hi: 142 times\n",
      "Top 3 words in May 2024:\n",
      "to: 632 times\n",
      "the: 631 times\n",
      "is: 522 times\n",
      "Top 5 busiest 5-minute intervals in May 2024:\n",
      "                        5min  message_count\n",
      "59 2025-09-01 20:00:00+02:00            344\n",
      "50 2025-09-01 19:15:00+02:00            288\n",
      "58 2025-09-01 19:55:00+02:00            274\n",
      "61 2025-09-01 20:10:00+02:00            269\n",
      "3  2025-09-01 15:20:00+02:00            248\n",
      "Stream with the highest messages per minute:\n",
      "   stream  message_count                start_time                  end_time  \\\n",
      "0     407           8622 2025-09-01 15:06:09+02:00 2025-09-01 20:15:55+02:00   \n",
      "\n",
      "   duration_min  messages_per_min  \n",
      "0    309.766667         27.833853  \n",
      "Chatters who chatted in every stream in May 2024:\n",
      "['0b3r0n1257', '0eivr', '11vann', '123lecaire', '143143143143143143', '1SKELTON', '1_3ka', '1pacificus_guy', '2Sh1n', '2faceura', '2vid_', '4bolfazl', '4xcocox2', '5070ti', '50Quid', '55Dash', '72number', 'ABSOLUTE__UNIT', 'ARTiL1S', 'Adelsc0tt', 'Akin_36', 'AkumoO_cs', 'Alkariusz', 'Allxdx', 'Aloddin', 'Aluminiumminimumimmunity', 'Alwaysinvisible', 'AmonixSmoke', 'AnAnonymousGifter', 'Angelss27', 'Anto_protoo', 'Arctic_Fin', 'BLAS1905', 'B_Mythical', 'BenXBari', 'BishopOfAngst', 'Bonescyth', 'BossBroocky', 'Bradva7', 'BrightrrSky', 'BruceWayne2211', 'Burke343', 'CRenn27', 'CZORTkekw', 'C_Moutinho', 'Chakarol', 'Columbidae_Kalla', 'Complexiity6', 'Constrickt0r', 'Cristi_NCT', 'CtrlAltDefeat___', 'Cuhhsin', 'DDogDxn', 'Dangerthoo', 'DenkenGG', 'DonMascarpon', 'DoorNob123', 'Drizzie3S', 'Dyhrrrr', 'Egoror8', 'Ehas__', 'Einau9e', 'Einstyle', 'ElPingou1', 'EleoodoR', 'Fauhacz', 'Fein_BV', 'FoLLeGG', 'Footstep_', 'Fossabot', 'Fr3on_Gaming97', 'Fujibayoshi', 'GOOSCHE_', 'Geminochio', 'GeneraISnus', 'Georgie1471', 'GladwinIsrael', 'Goose_Wearing_A_Hat', 'Groslouzer', 'Grubler4', 'GunjanBaba', 'Gunner_Pawn', 'Gusinic', 'HALP____', 'HaaMik', 'HakesW', 'Halmaz', 'HanSim_', 'Hazza_aoe', 'Hiexy21', 'HitMarkerino', 'Hueqi', 'I3ieLs_', 'IDKpandu', 'Iiidxnm12iii', 'ImTheMRL', 'InverseEntropy_', 'ItsAruka', 'ItsMatee__', 'ItsMeJeronimo', 'IvanOnMyOwn', 'Ivana_10', 'Ivy0001x', 'J4King', 'JBIN2036', 'JalleFalle', 'James__Fitzgerald', 'Janoszz', 'JippLe0', 'JohnCenaaaaaa_', 'Jolly___King', 'JotaPGS', 'JulesJ_', 'Just_a_zorg', 'Justlc0', 'KabutoWow', 'Kady86', 'Kappa_M1ke', 'Keruu_20', 'Ketchup_Assassin', 'Kleryan', 'Koki_Burchardt96', 'KrieSeaX', 'Kshvyy', 'Laysze', 'LettersRMZ', 'LilViktor22', 'Limbo_112', 'Losik66', 'LoupinStormborn', 'LuckyLukeStreams', 'M4RaZz', 'M8_Virtuoso', 'Mabathon', 'MacSandw1ch', 'Malbororreedd', 'MarsTwinkie', 'MasrurLOL', 'Maxilla_83', 'Maxyz777', 'Mefibus', 'Mons_Mardini', 'MonsieurSari', 'Morium28', 'Name_me_X', 'NeazzzY', 'NewWorld_FC', 'NexMorb', 'Nexurl', 'Nightbot', 'NikkelzeN', 'Nirrram', 'Norlandzz', 'NotBoshy', 'OMPAMA', 'Odah_02', 'Oslaf', 'PaSHaQviC', 'Pakukku', 'Parzival__2', 'Petittponeyy', 'PhilipFentanyl', 'PiGE0N98', 'PkyBlinderDrLenz', 'PotatoSwag98', 'Przemkowsky', 'Purplee010', 'Qewiii', 'R1cone91', 'RADARI_', 'Ravenbtw', 'RayChantilly', 'RedderzTV', 'RelmorX', 'RichTheMenace999', 'Ricix', 'SPLIFFERT', 'Sailx', 'Salii', 'SchiKen44', 'Schingledinger', 'Schleem', 'Sekiro47', 'ShadowHussein', 'Shazeh', 'SheenMean', 'Sololeveling99', 'SoyBeanSaucee', 'Stafeeev_', 'StanIV4_', 'Sticky_Pears', 'Stormhill', 'StrappingYT', 'Straydski', 'StreamElements', 'Substen', 'Sujkic_', 'SvenGilbertSilverdrake', 'Sypek_Wariat', 'Syptmpos', 'T_A_I_N_I_K', 'Techno_3', 'Terrale', 'TheAhmedGG', 'TheGreatSacatin', 'TheOldSeer', 'TheWorldNeedsaHero_', 'ThexErik_', 'TimDontMiss', 'Tinki95', 'To3sty', 'Trickboost', 'Twitchdivinity', 'Typhu25', 'Tyuns7', 'V1tALiTy15', 'VagabonddXD', 'VincentVonBroccoli', 'Violetters_', 'W1r3lesss', 'WEMASTMOOVE', 'Wanderer039', 'Warma99', 'WiXE__', 'Wilhelm77A', 'Wolf_szs', 'WorldVision__', 'Wysam', 'Yodawalk', 'Yomz1x', 'Zabimarru', 'ZelenyMegatron', 'Zeololz', 'ZephhWasTaken', 'Zoriemi', 'aaaminov', 'abbzau', 'abdellahmrxx', 'abdullaho15', 'abjectuz', 'adcristin', 'advocated1', 'aeternativ', 'aga_58', 'agenthercules25', 'agtonn', 'ahmwtsx', 'ahorsewithnonam3', 'ajfern_2890', 'ajjaysunuwar', 'akenne16', 'akezzez', 'alansmustche', 'alaqonquete', 'alcaponeonthisbtch', 'alek_1212', 'alensiii', 'aleprivki', 'alew_____', 'alkanized', 'alterverwalter01', 'ambitionhiro', 'amirmasoud_2018', 'andeas', 'andrea4V', 'arcanasasu', 'arksh001', 'arsene144', 'artemiy_v_vannoy203', 'asadbek_think', 'asierskalsooft', 'asznee49', 'atlantis_rare', 'attar_00', 'audrey_w_wolf1150', 'auraharvester_', 'averagebruinsenjoyer', 'awxtt', 'axnnxrx', 'ayleraylerayler', 'aznspideyweb', 'bagercho', 'balintboss', 'bankai_it', 'banties_x', 'barteddy', 'barvenaa', 'basedSTi', 'bayrettin', 'beatphazer', 'belfador_', 'benix1811', 'benjax_dota', 'beritooooo', 'best_troll_', 'big_melons27', 'bloodseekercat', 'bluecheesesalad93', 'bofur', 'bolduc2004', 'boo1ml', 'bot_panzer', 'boykeklol', 'bronn_overhaul', 'cIosurez', 'cakezfart', 'canadadahgoat', 'cappydw', 'captainfarca', 'carb0nm', 'carlosmr14', 'cast_deluxe', 'cat_spammer', 'certifiedsender', 'cheddaredspekter', 'chochochannel', 'cholokush', 'chrysskl', 'cigarettes_after_milk', 'clakston', 'clemes9', 'codymcg7', 'corch0pan', 'cordozariii', 'cr7vaibhav', 'cringeuss', 'cuminizi', 'cuuna', 'cyberpavlo', 'cyka9088', 'czekoladeczka789', 'd__e__x__t__e__r', 'danielandres271', 'danielblack_ex13', 'danyz111', 'darkcarnage_98', 'darklegecy_pro', 'dazai_menace', 'dead2p2', 'death_incoming2k25', 'debeans', 'dejw1dex', 'denisderjeep', 'dewochka_s_kare', 'diegoll', 'difedo_', 'djashin228', 'djcawish', 'djgfbd', 'dmtrplays', 'dominik___99', 'donlorenz', 'doredadomidodado', 'dorozeamodssindspreu', 'dr3m3r20', 'drainganglazar', 'drmoumou2001', 'drshakib_', 'duduckCS', 'dumbfreshman', 'echo_the_poopoo', 'elluiti', 'enatyk', 'eonnt', 'ericeraptor', 'explorergalaxy2025', 'f333rd00n_', 'fFAT1H', 'fabulousfailfbfb', 'faisal3ziz7', 'fanatmaddyson', 'faviocristianot', 'fazendaz', 'fcukboii', 'fer_2210', 'finikkkkkk', 'flacko565', 'flagzy_twitch', 'floppa1g', 'fnc_is_calling', 'fourduk', 'franklydizzy', 'fratskop', 'fredmckwacz', 'ftmuri', 'fun_time0707', 'g43bf', 'gabrielzzq', 'gadsdgsd', 'galvaocs_', 'gasporco', 'giftson45', 'girl_pope', 'girlp420_', 'gizzmo8787', 'gorthor11', 'gossipboyz', 'gowadiyo', 'graftimo', 'greekviewer1', 'greyofline', 'gridy___', 'grimm_b0', 'grzybek_renovado', 'gulihruz', 'gxlberblitz', 'h0wrud3', 'h3x_hunt3r', 'haHAA_12_btw', 'hardtm06', 'harszet', 'hatnaa', 'haze_787', 'hazzarrd1', 'hellonr1', 'hellscape48', 'heman_show', 'hembris', 'hemoYOUON', 'hhasl11', 'hille666', 'hkeremo3', 'hondzu_', 'honneeeeyyy', 'hosyann', 'hotdog_eoka', 'houses1', 'howami69', 'howljenkinss', 'iGod__xx', 'i_Astronaut', 'ian_yangazin97', 'icrylab', 'idmlistener', 'ihahik', 'iherx0', 'ilyanes88', 'ilyvi', 'imboring77', 'imnotmarx', 'inconspicuouscrow', 'intodgame', 'inzhir', 'ioke', 'ireallylovebastille', 'issaboi89', 'issakhalil12', 'its3zwz', 'itsagnar', 'j1740bm', 'jackob017', 'jadofff', 'janatojebem', 'jarlzphoneticallycharles', 'jasonjr2411', 'javierdenver18', 'javierezekiel7_', 'javiigrr', 'jay_ran', 'jayscs', 'jckxk', 'jebko_z_lesa', 'jeshcorona', 'jesusitpedia', 'jiepenjanneke', 'jihad1_sami', 'joanl3', 'johanysb', 'johnmacttavish', 'johnnycage___', 'jordison90', 'joriian', 'jp_emperor_sunxiaochuan', 'jperez0025', 'justcozx', 'kadavarLT', 'kaishirozaki', 'kaneki_king77', 'kapa2011', 'ken_snts', 'khsoraxiiix', 'kikochavochili', 'killer2005pr0', 'kirRoyale27', 'kitiano_', 'klimzaa', 'kong232_', 'korsikoff', 'kostamudictv', 'kosticka22', 'kotd_sabiba', 'kravee21', 'kuceljah', 'kvasimo69', 'l_d7mii', 'l_silva4', 'lagzero_', 'laukendoto', 'le_on22_09', 'leang4zm', 'legalizelandmines', 'leifdonnachad', 'lemocello0', 'lerssislaikka', 'lightyagami690', 'likezzcs', 'loganxdr', 'lokar10', 'lolcpll', 'loneeuw', 'losthelos', 'lotusdti', 'lovehatetragedy1987', 'luckier11', 'ludenss_', 'ludola_x7', 'luftwaffeln56', 'luis_alce', 'luizfcknzs', 'lukezilla_', 'lvxivs', 'lweedee', 'm4estr3', 'maccaa7x', 'machtinderechte', 'madsfast', 'madsss2k', 'magedotexe', 'maggnetoman', 'malhacocs', 'masna_knaga', 'matibits', 'matt_heyo', 'meIIicks', 'mechnik_ya', 'melongrab69', 'menmaryjanee420', 'meownki', 'mewll0w', 'miimii_94', 'minami_ray95', 'mircadd', 'mirun____', 'mistera3bsalam', 'mmdkaneki1', 'molski555', 'moonrossee', 'moza420', 'mraihanyazid', 'mrcle7', 'mrgruszqa', 'mrlolsss', 'mtsvalo', 'mucusproducer', 'muhammed_cengizzz01', 'muhrip7', 'muratcan_23', 'musavarol', 'mustygunay52', 'mykola213', 'n1v2t', 'naqu2k', 'narrat_x', 'narxdd', 'nassim458962', 'naufaltresna', 'ne0_jinx', 'nekroz2k', 'nellyfacee', 'neo_69191', 'neonfiast', 'neuro_01', 'nightwings777', 'nikiidamikozaku', 'nishad_more1311', 'nix5en', 'noJokeee1', 'nobody072809', 'nonesuchsg', 'not_so_pretty_boi', 'noverrrr', 'npe_v', 'nsfwxxx', 'nuckyr6', 'official_stingray', 'oliox', 'om2r10', 'orelmazal67', 'originaltudy26', 'ossmanyakuza', 'ownrocker', 'packetloss_0824121044', 'pakalucapito30', 'patlakias34', 'paulhenrich', 'pecks90', 'pendr1x', 'peyBaka', 'pfftsmh', 'phatloadbuster69', 'phrank007', 'pirrelol', 'podiatric', 'polimpompis', 'polixgap', 'pp1xell', 'prettygirly4', 'proyectozed', 'pssyosrs', 'purplesquad09', 'pxh_83', 'qazi_khan', 'queenn1982', 'quimeeeey', 'r_vmire_z', 'rafa30___', 'ramenreee', 'rastachann', 'rautsi__', 'residentspeeltool', 'reyyyyyyy', 'robbiekillboy', 'rockajay', 'rockerulol', 'rohandot', 'rosszcsontlu', 'rozenbergs', 'rozzzzaa', 'rubenmor123', 'ruhulamin001', 'ryanstayslingtv', 's2eed95', 'salehmahmoud1', 'samansolo77', 'samoooel', 'sant1_2k', 'santoscatra', 'sarvarazimboev', 'sauronk1', 'schoooooo_', 'sh0kky', 'shaffeeRU786', 'shogalul', 'shyroth__', 'sicox17', 'siddyxgiddy', 'sigurd6679', 'sirLUPUS238', 'skiizor', 'skoupidi__', 'skyspirits1', 'slipnotgang', 'sluttycatdad', 'softarballtt', 'sola7fa', 'solnishkosun', 'soraku00', 'sorru_please', 'sorruuuu', 'soyelcompadre', 'spasin', 'specter_zone', 'sportstarweirdo', 'stanny781', 'starmax004', 'stinkepuhprinzessin', 'stork9231', 'stukastriker', 'sucramdude91', 'sudelnuppe98', 'sufi_71', 'supchc', 'super_ps5', 'supperotto', 'suuuf371', 'swawyz', 'swishyswashyahh', 'szutyika123', 'taadow_', 'taksior', 'taliom', 'tars_568', 'tatold_wicikiewicz', 'teavapiti', 'teedrop999', 'teneightyp1080p', 'tensei7k', 'thatOutsider', 'the_abdurashid', 'thebadlou', 'thebigdogjay', 'theexeend', 'thefurki', 'theonlyman0', 'thepepedude', 'thewhitewolf1815', 'thomasson___', 'tiberoo', 'timonandbumba12', 'timoo02', 'timp_l', 'tmxko', 'toe_nail_1999', 'tommygun9941', 'tonycurve', 'trek_x', 'ulltramillk', 'unigamerslayer', 'unknownkekw', 'urie21', 'uselessbag', 'utah023', 'vaenessaa', 'vegeta_ki', 'venca_01', 'vendettahere', 'vereb993', 'victomat', 'vigduc', 'vini767_', 'volvo2agent', 'w0jtustoja', 'w_uddhav', 'wanqmm', 'wavyspider__', 'wemes517', 'whoskatka', 'whyalwaysgreeen', 'willem_zzz', 'wishmeluck___', 'woop_slap20', 'wrinkly_skrawtum3', 'x3lfk', 'xLoOoTz', 'xNovaV8x', 'xlilorie1990', 'xo_troii', 'xshinobi00', 'xskippack', 'xxxdixon', 'yandln', 'yeetzBae', 'yeiiow110', 'yidmutludegil', 'yigithans7yg', 'yluvros', 'your_lover1234', 'yurpty', 'yusufb20', 'zReeZ_Angel', 'zSuspicious', 'zaid_r33', 'zamand6226', 'zawgyynn', 'zazaonthego', 'zeus_0300', 'zherox_43', 'zimonsps', 'zoid87', 'zorro0077', 'zx1eq', 'zxelrate_', 'zxkre', 'zymosz', 'ãƒƒãƒƒãƒƒãƒƒãƒƒãƒƒ', 'è¡€é”è˜‘', 'ì•Œëž˜ìŠ¤ì¹´í•´ë‹¬']\n"
     ]
    }
   ],
   "source": [
    "# Filter only May 2024\n",
    "may_2024 = data[(data[\"date\"].dt.year == 2025) & (data[\"date\"].dt.month == 9)]\n",
    "\n",
    "# Find the first message date for each user\n",
    "first_messages = data.groupby(\"user\")[\"date\"].min().reset_index()\n",
    "\n",
    "# Filter users whose first message was in May 2024\n",
    "new_chatters = first_messages[\n",
    "    (first_messages[\"date\"].dt.year == 2025) & (first_messages[\"date\"].dt.month == 9)\n",
    "]\n",
    "\n",
    "# Get the number of new chatters\n",
    "num_new_chatters = new_chatters[\"user\"].nunique()\n",
    "\n",
    "print(f\"Number of new chatters in May 2024: {num_new_chatters}\")\n",
    "stream_counts = may_2024['stream'].value_counts().reset_index()\n",
    "print(f\"Number of Streams in May 2024: {len(stream_counts)}\")\n",
    "print(f\"Number of Messages in May 2024: {may_2024.shape[0]}\")\n",
    "print(f\"Number of Users in May 2024: {may_2024['user'].nunique()}\")\n",
    "user_counts = may_2024['user'].value_counts().reset_index()\n",
    "user_counts.sort_values('count').tail(10)\n",
    "\n",
    "# Count number of messages per user in May 2024\n",
    "user_counts = may_2024['user'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "user_counts.columns = ['user', 'message_count']\n",
    "\n",
    "# Get the top 10 users\n",
    "top_10_users = user_counts.head(10)\n",
    "\n",
    "print(top_10_users)\n",
    "\n",
    "# Initialize a Counter to store emote frequencies\n",
    "emote_counter = Counter()\n",
    "\n",
    "# Go through each message and count emotes\n",
    "for message in may_2024[\"message\"]:\n",
    "    words = message.split()\n",
    "    for word in words:\n",
    "        if word in final_emote_list:\n",
    "            emote_counter[word] += 1\n",
    "\n",
    "# Get top 5 emotes\n",
    "top_5_emotes = emote_counter.most_common(5)\n",
    "\n",
    "print(\"Top 5 emotes in May 2024:\")\n",
    "for emote, count in top_5_emotes:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Count all words from May messages\n",
    "word_counter = Counter()\n",
    "\n",
    "for message in may_2024[\"message\"]:\n",
    "    words = message.lower().split()\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Get top 3 most common words\n",
    "top_3_words = word_counter.most_common(3)\n",
    "\n",
    "print(\"Top 3 words in May 2024:\")\n",
    "for emote, count in top_3_words:\n",
    "    print(f\"{emote}: {count} times\")\n",
    "\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "may_2024 = may_2024.copy()\n",
    "\n",
    "# Round timestamps to nearest 5-minute interval\n",
    "may_2024[\"5min\"] = may_2024[\"date\"].dt.floor(\"5min\")\n",
    "\n",
    "# Count messages per 5-minute interval\n",
    "message_counts = may_2024.groupby(\"5min\").size().reset_index(name=\"message_count\")\n",
    "\n",
    "# Get top 5 busiest 5-minute intervals\n",
    "top_5_fastest = message_counts.sort_values(\"message_count\", ascending=False).head(5)\n",
    "print(\"Top 5 busiest 5-minute intervals in May 2024:\")\n",
    "print(top_5_fastest)\n",
    "\n",
    "# Group by stream and compute message counts and time range\n",
    "stream_stats = may_2024.groupby(\"stream\").agg(\n",
    "    message_count=(\"message\", \"count\"),\n",
    "    start_time=(\"date\", \"min\"),\n",
    "    end_time=(\"date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Compute duration in minutes\n",
    "stream_stats[\"duration_min\"] = (stream_stats[\"end_time\"] - stream_stats[\"start_time\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Avoid division by zero\n",
    "stream_stats = stream_stats[stream_stats[\"duration_min\"] > 0]\n",
    "\n",
    "# Calculate messages per minute\n",
    "stream_stats[\"messages_per_min\"] = stream_stats[\"message_count\"] / stream_stats[\"duration_min\"]\n",
    "\n",
    "# Get the stream with the highest messages per minute\n",
    "fastest_stream = stream_stats.sort_values(\"messages_per_min\", ascending=False).head(1)\n",
    "print(\"Stream with the highest messages per minute:\")\n",
    "print(fastest_stream)\n",
    "\n",
    "# Get all unique stream IDs in May\n",
    "all_streams = set(may_2024[\"stream\"].unique())\n",
    "\n",
    "# Group by user and get the set of streams each user chatted in\n",
    "user_streams = may_2024.groupby(\"user\")[\"stream\"].apply(set)\n",
    "\n",
    "# Filter users who chatted in every stream\n",
    "active_every_stream = user_streams[user_streams == all_streams]\n",
    "\n",
    "# Get just the user names\n",
    "users_in_every_stream = active_every_stream.index.tolist()\n",
    "print(\"Chatters who chatted in every stream in May 2024:\")\n",
    "print(users_in_every_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b377f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Trending by growth vs July:\n",
      "                                                 emote  count_aug  count_jul  \\\n",
      "132                                             maxwin          5        1.0   \n",
      "245                                               MeRN         35       32.0   \n",
      "88                                            JermaSoy          1        0.0   \n",
      "391                                    RememberTheDays          1        0.0   \n",
      "402                                            HandsUp          2        1.0   \n",
      "300                                               gPls          1        0.0   \n",
      "66   spfLEAN:()wiltee_()tonyhawkproskater4:-:-:_FRE...          5        4.0   \n",
      "369                                          Thinking2          4        3.0   \n",
      "393                                             Waddup          1        0.0   \n",
      "388                                             SteerR          1        0.0   \n",
      "\n",
      "     growth  \n",
      "132     4.0  \n",
      "245     3.0  \n",
      "88      1.0  \n",
      "391     1.0  \n",
      "402     1.0  \n",
      "300     1.0  \n",
      "66      1.0  \n",
      "369     1.0  \n",
      "393     1.0  \n",
      "388     1.0  \n",
      "\n",
      "ðŸ‘¥ Trending by unique users in August:\n",
      "      emote  count_aug  unique_users_aug\n",
      "6        hi        139                70\n",
      "30      LOL        184                59\n",
      "44       gg         83                47\n",
      "323     bye         71                38\n",
      "67       qq         66                37\n",
      "39       WW        159                37\n",
      "195     SAJ        101                37\n",
      "28     OOOO        112                36\n",
      "62       o7         67                34\n",
      "329  matSad        103                32\n",
      "\n",
      "ðŸ“Š Trending by frequency per stream:\n",
      "        emote  count_aug  streams_used_aug  freq_per_stream\n",
      "36   thinking        185                 1            185.0\n",
      "30        LOL        184                 1            184.0\n",
      "104       sob        178                 1            178.0\n",
      "39         WW        159                 1            159.0\n",
      "6          hi        139                 1            139.0\n",
      "105    OMEYES        131                 1            131.0\n",
      "28       OOOO        112                 1            112.0\n",
      "329    matSad        103                 1            103.0\n",
      "195       SAJ        101                 1            101.0\n",
      "97     NOOOOO         85                 1             85.0\n"
     ]
    }
   ],
   "source": [
    "# Filter July & August 2025\n",
    "july_data = data[(data[\"date\"].dt.year == 2025) & (data[\"date\"].dt.month == 8)]\n",
    "august_data = data[(data[\"date\"].dt.year == 2025) & (data[\"date\"].dt.month == 9)]\n",
    "\n",
    "# --- Helper: count emotes (total, users, per stream) ---\n",
    "def emote_stats(df, final_emote_list):\n",
    "    counts = defaultdict(int)               # total occurrences\n",
    "    users = defaultdict(set)                # unique users\n",
    "    stream_counts = defaultdict(set)        # unique streams where used\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        msg, user, stream = row[\"message\"], row[\"user\"], row[\"stream\"]\n",
    "        words = msg.split()  # split by whitespace\n",
    "        for emote in final_emote_list:\n",
    "            if emote in words:  # now it only counts exact whole word matches\n",
    "                counts[emote] += 1\n",
    "                users[emote].add(user)\n",
    "                stream_counts[emote].add(stream)\n",
    "\n",
    "    \n",
    "    stats = pd.DataFrame({\n",
    "        \"emote\": list(counts.keys()),\n",
    "        \"count\": list(counts.values()),\n",
    "        \"unique_users\": [len(users[e]) for e in counts.keys()],\n",
    "        \"streams_used\": [len(stream_counts[e]) for e in counts.keys()],\n",
    "    })\n",
    "    return stats\n",
    "\n",
    "# Get stats for both months\n",
    "july_stats = emote_stats(july_data, final_emote_list)\n",
    "august_stats = emote_stats(august_data, final_emote_list)\n",
    "\n",
    "# --- Merge to compute relative growth ---\n",
    "merged = pd.merge(august_stats, july_stats, on=\"emote\", how=\"left\", suffixes=(\"_aug\", \"_jul\"))\n",
    "merged = merged.fillna(0)  # emotes not in July become 0\n",
    "\n",
    "# Relative growth (compared to July counts)\n",
    "merged[\"growth\"] = merged[\"count_aug\"] - merged[\"count_jul\"]\n",
    "\n",
    "# Frequency per stream (normalize)\n",
    "merged[\"freq_per_stream\"] = merged[\"count_aug\"] / merged[\"streams_used_aug\"].replace(0, 1)\n",
    "\n",
    "\n",
    "# Sort by different \"trending\" definitions:\n",
    "top_growth = merged.sort_values(\"growth\", ascending=False)\n",
    "top_unique_users = merged.sort_values(\"unique_users_aug\", ascending=False)\n",
    "top_freq_per_stream = merged.sort_values(\"freq_per_stream\", ascending=False)\n",
    "\n",
    "print(\"ðŸ“ˆ Trending by growth vs July:\")\n",
    "print(top_growth[[\"emote\", \"count_aug\", \"count_jul\", \"growth\"]].head(10))\n",
    "\n",
    "print(\"\\nðŸ‘¥ Trending by unique users in August:\")\n",
    "print(top_unique_users[[\"emote\", \"count_aug\", \"unique_users_aug\"]].head(10))\n",
    "\n",
    "print(\"\\nðŸ“Š Trending by frequency per stream:\")\n",
    "print(top_freq_per_stream[[\"emote\", \"count_aug\", \"streams_used_aug\", \"freq_per_stream\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f32453b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Trending Users by Growth vs July:\n",
      "                  user  count_aug  count_jul  growth\n",
      "109     JohnCenaaaaaa_         77        4.0    73.0\n",
      "28   AnAnonymousGifter         64       19.0    45.0\n",
      "760              zx1eq         43        1.0    42.0\n",
      "48        Constrickt0r         40        0.0    40.0\n",
      "93            IDKpandu         34        8.0    26.0\n",
      "754        zazaonthego         82       61.0    21.0\n",
      "620          reyyyyyyy         50       29.0    21.0\n",
      "363              eonnt         46       26.0    20.0\n",
      "307            cappydw         27        8.0    19.0\n",
      "737         xshinobi00         21        3.0    18.0\n",
      "\n",
      "ðŸŽ¨ Trending Users by Unique Emotes Used:\n",
      "           user  unique_emotes_used  count_aug\n",
      "104    JBIN2036                 157        386\n",
      "618    rautsi__                 141        427\n",
      "34     BenXBari                 123        757\n",
      "324  cr7vaibhav                 106        382\n",
      "83     HALP____                  87        319\n",
      "155     Odah_02                  78        234\n",
      "282  balintboss                  68        254\n",
      "162    PiGE0N98                  68        122\n",
      "615   rafa30___                  57        170\n",
      "189    StanIV4_                  54        106\n",
      "\n",
      "ðŸŸ¢ Trending Users by Streams Active In:\n",
      "                    user  streams_active  count_aug\n",
      "0             0b3r0n1257               1          1\n",
      "515            ludola_x7               1          7\n",
      "506             loganxdr               1          1\n",
      "507              lokar10               1          2\n",
      "508              lolcpll               1          1\n",
      "509              loneeuw               1          1\n",
      "510            losthelos               1          3\n",
      "511             lotusdti               1          1\n",
      "512  lovehatetragedy1987               1          1\n",
      "513            luckier11               1          2\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Count messages per user ---\n",
    "july_counts = july_data.groupby(\"user\")[\"message\"].count().reset_index(name=\"count_jul\")\n",
    "august_counts = august_data.groupby(\"user\")[\"message\"].count().reset_index(name=\"count_aug\")\n",
    "\n",
    "# Merge July + August\n",
    "user_trends = pd.merge(august_counts, july_counts, on=\"user\", how=\"left\").fillna(0)\n",
    "\n",
    "# Growth in messages\n",
    "user_trends[\"growth\"] = user_trends[\"count_aug\"] - user_trends[\"count_jul\"]\n",
    "\n",
    "# --- 2. Unique emotes used per user (August only) ---\n",
    "def count_unique_emotes(msgs, final_emote_list):\n",
    "    used = set()\n",
    "    for m in msgs:\n",
    "        words = m.split()\n",
    "        for em in final_emote_list:\n",
    "            if em in words:\n",
    "                used.add(em)\n",
    "    return len(used)\n",
    "\n",
    "unique_emotes = august_data.groupby(\"user\")[\"message\"].apply(\n",
    "    lambda msgs: count_unique_emotes(msgs, final_emote_list)\n",
    ").reset_index(name=\"unique_emotes_used\")\n",
    "\n",
    "user_trends = user_trends.merge(unique_emotes, on=\"user\", how=\"left\")\n",
    "\n",
    "# --- 3. Streams participated in (August only) ---\n",
    "streams_per_user = august_data.groupby(\"user\")[\"stream\"].nunique().reset_index(name=\"streams_active\")\n",
    "user_trends = user_trends.merge(streams_per_user, on=\"user\", how=\"left\")\n",
    "\n",
    "# --- Results ---\n",
    "top_growth_users = user_trends.sort_values(\"growth\", ascending=False)\n",
    "top_emote_variety = user_trends.sort_values(\"unique_emotes_used\", ascending=False)\n",
    "top_consistency = user_trends.sort_values(\"streams_active\", ascending=False)\n",
    "\n",
    "print(\"ðŸ“ˆ Trending Users by Growth vs July:\")\n",
    "print(top_growth_users[[\"user\", \"count_aug\", \"count_jul\", \"growth\"]].head(10))\n",
    "\n",
    "print(\"\\nðŸŽ¨ Trending Users by Unique Emotes Used:\")\n",
    "print(top_emote_variety[[\"user\", \"unique_emotes_used\", \"count_aug\"]].head(10))\n",
    "\n",
    "print(\"\\nðŸŸ¢ Trending Users by Streams Active In:\")\n",
    "print(top_consistency[[\"user\", \"streams_active\", \"count_aug\"]].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
