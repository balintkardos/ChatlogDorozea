{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb622cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pytz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b79b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the list of filenames from the configuration file\n",
    "with open('file_list.txt', 'r', encoding='utf-8') as config_file:\n",
    "    file_names = config_file.read().splitlines()\n",
    "\n",
    "# Regex pattern to match the data format\n",
    "pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "\n",
    "# Initialize an empty list to store parsed data\n",
    "datalist = []\n",
    "stream_count = 0\n",
    "# Iterate over each specified file\n",
    "for file in file_names:\n",
    "    full_path = \"data\\\\\"+file\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, user, message = match.groups()\n",
    "                datalist.append([date, user, message,stream_count])\n",
    "    stream_count = stream_count + 1\n",
    "\n",
    "# Create a DataFrame from the parsed data\n",
    "data = pd.DataFrame(datalist, columns=[\"date\", \"user\", \"message\",\"stream\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdea79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a924f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utc_to_cet(df, date_column='date'):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure timestamps are UTC aware\n",
    "    if df[date_column].dt.tz is None:\n",
    "        df[date_column] = df[date_column].dt.tz_localize('UTC')\n",
    "    elif df[date_column].dt.tz != pytz.UTC:\n",
    "        df[date_column] = df[date_column].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Convert to CET/CEST (Europe/Berlin includes proper DST handling)\n",
    "    df[date_column] = df[date_column].dt.tz_convert('Europe/Berlin')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d51fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_utc_to_cet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f766686",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"user\"] = data[\"user\"].replace(\"Banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"CHILI_POE\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_conbacon\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Wirelesss_\", \"W1r3lesss\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"treklul\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"ttrek_\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"trek_x\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"TriplesingleJ\", \"TripleSingleJames\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uwu_cougar\", \"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uuccugr_\",\"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"StanIV4_\", \"stan_iv4\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Muuskie2\", \"Muuskie\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"nishad_more1311\", \"nishad13\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballt\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballtt23\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"lajosbarnabas\", \"lajoss__\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Bonkwiththefunk\", \"bonk67\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e470038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique usernames\n",
    "unique_users = data['user'].unique()\n",
    "\n",
    "# Create a mapping from lowercase username to all variants\n",
    "\n",
    "user_variants = defaultdict(set)\n",
    "for user in unique_users:\n",
    "    user_variants[user.lower()].add(user)\n",
    "\n",
    "# Find usernames with different capitalization\n",
    "duplicate_users = {k: v for k, v in user_variants.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1453881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from all variants to the canonical (sorted first) variant\n",
    "variant_map = {}\n",
    "for variants in duplicate_users.values():\n",
    "    sorted_variants = sorted(variants)\n",
    "    canonical = sorted_variants[0]\n",
    "    for v in variants:\n",
    "        variant_map[v] = canonical\n",
    "\n",
    "# Replace usernames in 'user' column\n",
    "data['user'] = data['user'].apply(lambda u: variant_map.get(u, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5959b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine the \"First Seen\" date for every user\n",
    "# This tells us when a user originally joined the community\n",
    "user_birthdays = data.groupby('user')['date'].min().reset_index()\n",
    "user_birthdays.columns = ['user', 'first_seen']\n",
    "\n",
    "# Merge this back into the main dataframe\n",
    "df = data.merge(user_birthdays, on='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599dde53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chatters (Feb 2026): 4617\n",
      "Truly New Chatters: 1224\n",
      "Returning Chatters: 3393\n",
      "Percentage New Users: 26.51%\n",
      "\n",
      "✅ RESULT: Theory 2 is CORRECT (Mostly old regulars).\n",
      "\n",
      "Median Days Since Last Chat (for Returning Users): 16 days\n",
      "Insight: Old users were already active recently. They are just hardcore regulars.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 1: RESET & CLEAN\n",
    "# ---------------------------------------------------------\n",
    "# Start fresh from your original 'data' variable to avoid _x/_y errors\n",
    "df = data.copy()\n",
    "\n",
    "# Fix the Date format and remove Timezones (Fixes the TypeError)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "if df['date'].dt.tz is not None:\n",
    "    df['date'] = df['date'].dt.tz_localize(None)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: CALCULATE \"FIRST SEEN\"\n",
    "# ---------------------------------------------------------\n",
    "# Find the earliest date for each user\n",
    "user_birthdays = df.groupby('user')['date'].min().reset_index()\n",
    "user_birthdays.columns = ['user', 'first_seen']\n",
    "\n",
    "# Merge into the main dataframe\n",
    "df = df.merge(user_birthdays, on='user', how='left')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: ANALYZE FEB 2026\n",
    "# ---------------------------------------------------------\n",
    "feb_start = pd.Timestamp(\"2026-02-01\")\n",
    "feb_end = pd.Timestamp(\"2026-03-01\")\n",
    "\n",
    "# Filter for Feb 2026 streams\n",
    "feb_df = df[(df['date'] >= feb_start) & (df['date'] < feb_end)].copy()\n",
    "\n",
    "# Get unique users active in Feb\n",
    "# We use drop_duplicates on 'user' to get 1 row per chatter\n",
    "active_users = feb_df[['user', 'first_seen']].drop_duplicates()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 4: PROVE THE THEORIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Theory 1: \"True New\" Users (First seen inside Feb 2026)\n",
    "new_users = active_users[active_users['first_seen'] >= feb_start]\n",
    "\n",
    "# Theory 2: \"Returning\" Users (First seen before Feb 2026)\n",
    "returning_users = active_users[active_users['first_seen'] < feb_start]\n",
    "\n",
    "print(f\"Total Chatters (Feb 2026): {len(active_users)}\")\n",
    "print(f\"Truly New Chatters: {len(new_users)}\")\n",
    "print(f\"Returning Chatters: {len(returning_users)}\")\n",
    "\n",
    "# Calculate Ratio\n",
    "new_ratio = len(new_users) / len(active_users) if len(active_users) > 0 else 0\n",
    "print(f\"Percentage New Users: {new_ratio:.2%}\")\n",
    "\n",
    "if new_ratio > 0.5:\n",
    "    print(\"\\n✅ RESULT: Theory 1 is CORRECT (Mostly new audience).\")\n",
    "else:\n",
    "    print(\"\\n✅ RESULT: Theory 2 is CORRECT (Mostly old regulars).\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 5: DEEP DIVE ON THEORY 2 (Dormancy)\n",
    "# ---------------------------------------------------------\n",
    "if len(returning_users) > 0:\n",
    "    returning_ids = returning_users['user'].unique()\n",
    "    \n",
    "    # Look at history only for these specific users BEFORE Feb 2026\n",
    "    # Note: We use 'df' here to see their full history\n",
    "    history = df[(df['user'].isin(returning_ids)) & (df['date'] < feb_start)]\n",
    "    \n",
    "    if not history.empty:\n",
    "        # Last time they spoke before Feb\n",
    "        last_seen = history.groupby('user')['date'].max()\n",
    "        \n",
    "        # Calculate gap in days\n",
    "        gap_days = (feb_start - last_seen).dt.days\n",
    "        median_gap = gap_days.median()\n",
    "        \n",
    "        print(f\"\\nMedian Days Since Last Chat (for Returning Users): {median_gap:.0f} days\")\n",
    "        \n",
    "        if median_gap > 30:\n",
    "            print(\"Insight: Old users were 'dormant' for a long time. The new time likely woke them up!\")\n",
    "        else:\n",
    "            print(\"Insight: Old users were already active recently. They are just hardcore regulars.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
