{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1d5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b8033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of filenames from the configuration file\n",
    "with open('../file_list.txt', 'r', encoding='utf-8') as config_file:\n",
    "    file_names = config_file.read().splitlines()\n",
    "\n",
    "# Regex pattern to match the data format\n",
    "pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "\n",
    "# Initialize an empty list to store parsed data\n",
    "datalist = []\n",
    "stream_count = 0\n",
    "# Iterate over each specified file\n",
    "for file in file_names:\n",
    "    full_path = f\"../data/{file}\"\n",
    "    with open(full_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, user, message = match.groups()\n",
    "                datalist.append([date, user, message,stream_count])\n",
    "    stream_count = stream_count + 1\n",
    "\n",
    "# Create a DataFrame from the parsed data\n",
    "data = pd.DataFrame(datalist, columns=[\"date\", \"user\", \"message\",\"stream\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd82712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258c8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_utc_to_cet(df, date_column='date'):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure timestamps are UTC aware\n",
    "    if df[date_column].dt.tz is None:\n",
    "        df[date_column] = df[date_column].dt.tz_localize('UTC')\n",
    "    elif df[date_column].dt.tz != pytz.UTC:\n",
    "        df[date_column] = df[date_column].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Convert to CET/CEST (Europe/Berlin includes proper DST handling)\n",
    "    df[date_column] = df[date_column].dt.tz_convert('Europe/Berlin')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd1d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_utc_to_cet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58637951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"user\"] = data[\"user\"].replace(\"Banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"banties1g\", \"banties_x\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"CHILI_POE\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Chili_poe\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"chili_conbacon\", \"chili_con_bacon\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Wirelesss_\", \"W1r3lesss\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"treklul\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"ttrek_\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"trek_x\", \"trek44_\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"TriplesingleJ\", \"TripleSingleJames\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uwu_cougar\", \"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"uuccugr_\",\"uuccugr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"StanIV4_\", \"stan_iv4\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Muuskie2\", \"Muuskie\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"nishad_more1311\", \"nishad13\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballt\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"softarballtt23\", \"softarr\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"lajosbarnabas\", \"lajoss__\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"Bonkwiththefunk\", \"bonk67\")\n",
    "data[\"user\"] = data[\"user\"].replace(\"qfishyy11\", \"bonk67\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1281648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all unique usernames\n",
    "unique_users = data['user'].unique()\n",
    "\n",
    "# Create a mapping from lowercase username to all variants\n",
    "\n",
    "user_variants = defaultdict(set)\n",
    "for user in unique_users:\n",
    "    user_variants[user.lower()].add(user)\n",
    "\n",
    "# Find usernames with different capitalization\n",
    "duplicate_users = {k: v for k, v in user_variants.items() if len(v) > 1}\n",
    "\n",
    "# Create a mapping from all variants to the canonical (sorted first) variant\n",
    "variant_map = {}\n",
    "for variants in duplicate_users.values():\n",
    "    sorted_variants = sorted(variants)\n",
    "    canonical = sorted_variants[0]\n",
    "    for v in variants:\n",
    "        variant_map[v] = canonical\n",
    "\n",
    "# Replace usernames in 'user' column\n",
    "data['user'] = data['user'].apply(lambda u: variant_map.get(u, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1919eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique emotes: 1040\n"
     ]
    }
   ],
   "source": [
    "# 7tv list\n",
    "tv7_emotes = \"\"\"omeFaded nonono wideSpeedFear wideSpeedDesert SpeedOmg Disappointed ME? ome13 doroPfft CONFUSED doro52 wideSpeedConcerned doroBANGER doroVibe DoroBrainPLug NAHHHH intrigued oda WOOF BANG peepoFirework Gladge omeMoji SobBounce belka doroHips dorochan doroXi DOMEYES DOWAY FRENCH YeahIGetIt doroPls Offline sheMightBeRight 89 dwerk wideGkeyDance3 76 xqcCheer omeCheer Dmile zoro LMAO iguessbro doroScary haii SON omeBru wideSpeedCall plink laracroft ADHD yipe doroWaiting 67 FOWL doroLOL OfCourse tyler67 THYME peepoLost speed61 GOODMOOD Jercula ILOVECS CLOWN thatsCrazy xqcBleh omEING wideSpeedLaugh15 WHATTHEHELI RickyDicky jonkler wideSpeedPumpkin wideSpeedLaugh16 SPIDER chillCat LEEKED soblaugh BANANA wideSpeedLaugh21 Binoculars footstepmenace ome99 doroBanger GhankYou NO YES auntiePls OMEGADANCEBUTGHAST gkeyRide GENERATINGGODSEED perimeter sideeye omePfft MySunshine posture ??? twocatsfightingonacouch dSmile wideSpeedLaugh4 flirtt Suspicion zigzag YeatCat nom omeSad shomonting thinking agafat FirstTimeClanka DORVIS gasp RISING dd uncPLS domgBruh Deafge OMEGADANCEBUTFASTER kim3 Buggin speed8 .... d32 ome54 ohok minionBike Clown Explosion hackingCD JermaSoy MathTime MoneyRain PokiShare TakingNotes :0 :3 :33 :tf: !boost +1 0pixel 1DLove 3Head 3Heading 4House 4Shrug 4Weird 5Head AAAA Acknowledged ACTINUP ADHD agahi AIM AIRBALL AIWITHTHEBRAIDS Alarm ALE Alfred AlienDance AlienPls AlienPls2 AlienPls3 Aloo Alright amongE ANGRE ANOTHERONE Ant AREYOUAGIRL AREYOUAGIRLFtxQcYellingAtYou areyoufr AreYouSeriousRightNeow arnoldHalt arthur Assept AURA AwHellNah Aware AWOO AWOOGA axelF ayo bah BAND Banger BANGER banties Barack barryArrive Barry63 BantiesPaulBeef Based BASED BatChestAbove batman Batman batJAM batPls Beatles Bedge BEG BEGGING Bello BigD bieberDougie BELIEVERS BibleThump BINGO Bleh Bloons BOOBA bog BOOM BOOMIES BOINK BORGIR Borfday brbToilet Broadcaster brb Bruh BRUHMM bruv buh buhbye buhFlipExplode BUSSIN BUSSERS bye CanIHaveADollar cannySilly catAsk catBusiness CatCozy catDespair catEat catJAM catKiss catPls catSigh catSmash CatTime catTwerk CAUGHT Caught CaughtIn4K Celebrating CHADDING characterSelected CHATTERS chatting cheerleaders ChillGuy Chillin Cinema chilling clappi Clap classic CLEAN Clueless CLIPPERS CLOWNDETECTED COCKA cokeBreak COMEHERE Concerned Considering Cooked COPIUM crabPls Crunch CS2 Cuck Cuh D: damily Damn dansi dash Dave deadassFaint Delusional DemonTiming Dentge despair DespairRyan Devious DIESOFCRINGE Dime DinkDonk Dinema doggoSlava dogJAM DogLookingWickedAndCool doid dojaPls dome44 dome32 donowall donoWall doroAunt doroBleh dorobubu doroCD DoroCheer doroFiddy doroFlex doroGHOST doroHEAD doroKick doroL doroMAD doroPray doroRip doroSoy DoroTalkingAgain Dorozea doster DOUBTERS DRAIN Drake DRAMA dreamwastaken drooling drukiDnace drukiDnace2 duaKiss dudWhat EDGE EDM EDITING emo erinNya essaying ewphop eww EZ EZdodge Exerpas Explosion eyeroll fadedthanaho FARMING FeelsBadMan FeelsDankMan FeelsBlackScreen FeelsGladMan FeelsLagMan FeelsLateMan FeelsOkayMan FeelsStrongMan FeelsTiredMan FeelsWeirdMan FeelsWowMan fein FEINFEINFEINFEINFEINFEINFEINFEI FellOff fembajJAM Fiddy FiddyWtf FINALLY firewriting FirstTime FirstTimeBackseating FirstTimeChadder FirstTimeChatter FirstTimeEmoteFail FirstTimeGooner FirstTimePepega FirstTimeTest firsttimebuh FLASHBANG flightnotL Flirt Flushed fnaf footstep forsenCD forsenLaughingAtYou ForsenSingingAtYou forsenPls fortnite fr freakbob freakyfredday freddy Freedom FUNNY g32 GAGAGA gachiGASM gachiHYPER gamily GAMBA GameplayTime GAMING GatieG Gaught GENIUS GetALoadOfThisGuy gg GIGACHAD GIGACHAIR GIGACLAUS GIGAMODS GIGAMOD gigl gkeyFlip gkeyPregnantBounce gkeySMP gkeyUwu gkeywide gkeyWiding gkitten GivenUp girlBoss gkitten glorpaga glorpdetective glorp GlorpMeeting glorprave gmoney goaler goat goblin44 GODDID Gogging GoodBye Gooner gooner GoodTake GOONING GotCaughtTrolling GotEEM gothKiss gPls greetingsladies GREEDY GROOTING GRRR GULP GuitarTime GYAT HABIBI hackingCD HACKERMANS hai HAH HaltEinfachDeineFresseDuHurensohn HandsUp happi HARAM HarryStylesKiss Headbang healed HECOOKING heh HEHE HEHEHEHA Heisenberj HELLO HELP helvete Herewego hesRight heyywithrizz HEYYY hi hiii hiiii Hmm HOBBY HOLY HolyFuck homelessPOV HowDoWeTellHer HowDoWeTellHim hue HUH HUHHHHHHHHHH iAsked ICANT idiot iDrive IFISPEAK IfYouCantSeeThisEmoteUseExclamationMark7tv Ignored IGON imback IMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGETFUCKEDNON7TVUSERSIMAGINENOTHAVING7TVGET ImNotOk ImtiredBoss INTENSEGAMING islandboy ISeeYou itsover itstime Jackass jacob1 jacob2 jacob3 jacob4 jah Jammies JARVIS Jay JermaSoy jiggy job JOB Joel joever john Johnporkiscalling JokerHAHA JokerLaugh juh JumpScared JUMPSCARE JustAChillGuy JustAnotherDay JustHowItIs justinbieber KaiCenatOhiogyatwithskibiditoiletwatchingtheWrizzhappeningrightinfrontofhimwithfanumtaxtaxingthegyat KanyeStare KEKW KENOUGH KeyShaker kim3 kittyBANGER kittyBop KKalinka KKonaW KKool kratos Lamonting LastTimeChatter lava lebronArrive lebronJAM lebronTROLL LEBRONNN lemon Lemon LetsBingo LETHERCOOK LETSFUCKINGJOE LETSGO LieMeter life Life Listening LiterallyMe Lithuanian LittleTrolling LiveReaction LL LMAOFREAKY lmao Loading LOCKIN lockedin LOL Looking LookUp lore luh lurkk luton LULE LULW MAJ Madge ManchesterUnited Massive? MarblesTime Martin matSad maxwin MeRN me: MeWhenIBuyEgyptianProperty MEGALUL mee Memories merch mhm MicTime mikuPls mindloud modCheck ModAbuse MODDING Modding mods MODS Mog monakS monday monkeyListening monkeySip MONKA monkaTOS monkaW MONKE MinionHoting MoneyRain muted mutted MUGA MVP MVPOfFarallah MYHEARTILOVEDHER myIQ MYLIFE NAILS NAILSING NAHH NAHHH NAHHHH nananAYAYA NAUR NAvsEU Nerd niceguy NOCHECKMARKS NODDERS NOIDONTTHINKSO NoMaidens NOOPERS NOOOOO NOHORNY noonecares NOSHOT NOTED notListening notxqcL NOW NOWAY NOWAYING np nt nuhuh nyehehehe nyanPls nya o7 Ogre ohhh ohhhhhhhhh OHMYGAWDD ohneFinger ohno ohSHIT oj OK Okei okak OLDWORK OM om omE ome10 ome101 ome104 ome105 ome14 ome15 ome18 ome21 ome29 ome32 ome4 ome41 ome44 ome44444444 ome47 ome5 ome51 ome52 ome55 ome57 ome67 ome69 ome79 ome808 ome83 ome9 ome90 ome96 ome99 OMEGADANCE OMEGALUL OMEGALULiguess omEE omeJAM omeJudging omeOhSHIT omeScrajj omeStare OMEYES omeWiggle OMFG omgBruh ongang OneGuy ONEMORE OnMyWayToDoroMomHouse OOOO oopsie otag OuttaPocket OVERWATCH OVERWORKING OverwhelminglyWholesome owoCheer PagBounce PagChomp PagMan Panam parasocial Parasocial PARASOCIAL paris paul Paul paulNya PauseMan PAUSENEMOGU Peace PEEPEES peepoAds peepoBox peepoBelievers peepoClap peepoComfy peepoDJ peepoDoubters peepoEvil peepoFarmer peepoFat peepoGiggles peepoHappy peepoHey peepoHug peepoKiss peepoLeave peepoLegs peepoLove peepoMarch peepoPls peepoPride peepoRiot peepoSad peepoShy peepoSmile peepoStop peepoTalk pepeAgony pepeGun PepeHands pepeJAM PepeLaugh pepePoint PepePls pepeW Pepega PepegaAim PepegaChat PepegaReading PepoG Petter Pffttt Pffttt2 phew phpk pickle PianoTime Pipege pKitten pL Please pleading plink-182 plinkVibe plonk pmo Plotge PogO PogU pointless pokiFlirt pol POLICE Pondering popipopipipopipo poroPls POVbornbefore2000 ppHop ppL ppOverheat Prayge prePffttt PRIMERS PTSD pulNya PuzzleTime qq ragebait RAGEY RAHH RainTime RAMBOLMG RareParrot ratomilton RaveDance RaveTime ratio Reacting RealForsen ReallyMad RebeccaBlack Reddit RememberTheDays RibertJam RiddleMeThis RIPBOZO RIRI Rizzler RobertJam ROFL RoxyPotato RUNNING rt ryanArrive Sadding Sadge SADge SAJ SAVEME SCATTER saythatagain scawy SCHEISSE SCHIZO SCRAPETHATSHITJOHNNY SCHTOP sdd SERIOUSLY SEXO shogaNya Shits shutup Shruge silliness sisyphus Sippin Sits skip SLAY Sleepo Smile smh Smoge SmurfHey Smurfing SNACKING SNEAK SNIFFA sob SOLARFLARE songbird sotruebestie SOYSCREAM Speechless speed1 speed2 speed21 speed25 speed32 speed4 speed44 speed8 speed88 speedVibe spfLEAN:()wiltee_()tonyhawkproskater4:-:-:_FREEWAVE3-encinoman--:enteringwalmart:-wheezethelean-123 SpeedLaugh SpeedLeft SpeedR spongePls squadHips Stare Staring steve Steve SteerR StreamEnding STREAMER STREAMERSGIVINGTHEWORSTFUCKINGTAKESINEXISTENCE StoryTime Surfing SurE sus susDog Susge SUSSY Swag swagJAM ta tak TakingNotes TeamEDWARD test THATHIT ThatsJustMe ThePaulers TheVoices TheWolfInMe Thinking Thinking2 ThisChat ThisIsMinecraft TIMEOUT Tomfoolery totallylistening TriJam TriKool TriSad TRUEING TRIVSsorry ts Tuckge tuff TWEAK typeshit typhu UGH um UltraMad unibrow unemployment unmod uwu uuh VALORANT veryDoro VeryKey VeryPog VeryPogftxQcInTheShower vibePls VibePls VIDEOGAME VIEWERS vips Voices wade Waddup waga wah waiting Waiting WAHHH WAJAJA WAIT WAITWAITWAIT WakeTheFuckUpSamuraiWeHaveACityToBurn wallE waltuh walterShocked WalterVibe War WasZumPenis WATAFUCKEDUPDAY WatchingStream WAYTOODANK wdym WeAreLive WeDoNotCare WEDIDIT WEEWOO WeGood WePaid WHATAFUCKEDUPDAY WHAT WHATTT wheresmyhug Whenyourinnerwolfreleases WideAlERT WideCatGroove wideDvaAss WideHardo WidelebronJAM widemonkaGIGAftRobertDowneyJr wideprespeedlaugh WideRaveTime wideReacting wideSpeedLaugh3 widetime WidezyzzPls wig WineTime winton Wisdom woah Wokege WOT wot wrapitup WW wowie Xd xar2EDM xdd XDoubt xJAM xqc32 xqcBOZO xqcDespair xqcFuel xqcGoofy xqcL xqcSCHIZO xqcSlam xqcTake xqcTwerk xQcVeryWide YAAAY YamesBond YANITED YAPPING YeahThatsWhatIWouldaDid YEAHHH YEP YESS YIPIEE yonose Yoink YOOLOOKATTHISCATDOINITSLILDANCYDANCEINTOABREAKDANCEMOVE Yooo YOUDIED YouGotMe YouWouldntGetIt ZAMN ZhongXina zyzzBass zyzzJAM\"\"\"\n",
    "\n",
    "# Convert to list and clean \n",
    "emote_list = [emote.strip() for emote in tv7_emotes.split() if emote.strip()]\n",
    "\n",
    "# Remove any remaining duplicates (if any)\n",
    "unique_emotes = sorted(list(set(emote_list)))\n",
    "\n",
    "print(f\"Total unique emotes: {len(unique_emotes)}\")\n",
    "\n",
    "# Create the final shortened list\n",
    "final_emote_list = unique_emotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e85d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emotes = [\n",
    "    'ome62','Staredown','PentagramOfFarallah','Gloving','footgammaRadiation',\n",
    "    'RAAAAAAAAGH','praise','doroRage','AndreSmithing','omePIECE','Rime',\n",
    "    'doroPIECE','kaiReading','Deadge','furi','blub','duh','nuh','gopissgirl',\n",
    "    'plong','Awesome','o','feaky','DoroThinking','evol','doro18','staycalm',\n",
    "    'omeFOWL','s','fairs','deal','moshimoshi','gn'\n",
    "]\n",
    "\n",
    "final_emote_list = list(set(final_emote_list) | set(new_emotes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4028e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emotes = [\n",
    "    'ae','doroModCheck','Sup','DOSESHEKNOW','BENNED','bitrate','cortisolspike','mlem','catWait','gkeyWave','paulPls','monkeyPls','mountainlion','Walking','KILLTHATBOY','SenChest','peculiar','Senny','wideSpeedPoint','Emotional','sayWALLAHI','doroPls2','sennyTurkey','paulPiece','SCRAPETHATSHITDORO'\n",
    "]\n",
    "\n",
    "final_emote_list = list(set(final_emote_list) | set(new_emotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635dd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Excel file\n",
    "file_path = '../DorozeaStreamStats.xlsx'\n",
    "\n",
    "# Reading a specific sheet with custom column names and skipping the first two rows\n",
    "steamStats = pd.read_excel(file_path, sheet_name='Munkalap1', header=None, names=['stream', 'avg', 'max','follow','games'], skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6b12a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LA NOIRE (156-166) ARC ===\n",
      "Total Duration: 2 days 14:12:11\n",
      "Total Messages: 61168\n",
      "Unique Chatters: 3629\n",
      "First-Time Chatters: 1727\n",
      "Avg Message Speed: 16.39 messages/min\n",
      "Total Follows: 1442\n",
      "\n",
      "Top 10 Chatters:\n",
      "user\n",
      "elluiti           2697\n",
      "JBIN2036          2570\n",
      "CrazeE420xd       1612\n",
      "Martin_Gales      1533\n",
      "trek44_           1484\n",
      "balintboss        1451\n",
      "riesenklotz       1305\n",
      "catsspurr         1098\n",
      "StreamElements    1044\n",
      "fuzok4            1009\n",
      "\n",
      "Top 10 Emotes:\n",
      "message\n",
      "ome44     8085\n",
      "LOL       1444\n",
      "OOOO      1078\n",
      "mhm        899\n",
      "hi         866\n",
      "WW         769\n",
      "ome18      581\n",
      "omE        523\n",
      "ome5       504\n",
      "Prayge     496\n",
      "----------------------------------------\n",
      "\n",
      "=== LA TRIP (538-558) ARC ===\n",
      "Total Duration: 2 days 09:28:27\n",
      "Total Messages: 105985\n",
      "Unique Chatters: 5744\n",
      "First-Time Chatters: 1662\n",
      "Avg Message Speed: 30.73 messages/min\n",
      "Total Follows: 1073\n",
      "\n",
      "Top 10 Chatters:\n",
      "user\n",
      "FilipStayout    5794\n",
      "Odah_02         4741\n",
      "polimpompis     4551\n",
      "erdeedge        4418\n",
      "rafa30___       4248\n",
      "JBIN2036        3531\n",
      "BenXBari        3501\n",
      "Swimtowin1      2825\n",
      "nishad13        2822\n",
      "lajoss__        2808\n",
      "\n",
      "Top 10 Emotes:\n",
      "message\n",
      "hi        3430\n",
      "WW        2900\n",
      "sob       2589\n",
      "LOL       2422\n",
      "ome44     1688\n",
      "mhm       1579\n",
      "bye       1427\n",
      "OOOO      1347\n",
      "LMAO      1306\n",
      "OMEYES    1202\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. PREPARATION ---\n",
    "# Make sure your date column is actually a datetime object so we can calculate durations\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Convert the emote list to a set for much faster lookup speeds\n",
    "emote_set = set(final_emote_list)\n",
    "\n",
    "# Define the stream ranges (Python's range is exclusive at the end, so add 1)\n",
    "la_noire_streams = list(range(156, 167)) # 156 to 166\n",
    "la_streams = list(range(538, 559))       # 538 to 558\n",
    "\n",
    "# Pre-calculate the absolute first stream for EVERY user in the entire dataset\n",
    "# This is needed to figure out true \"first-time chatters\" for an arc\n",
    "user_first_stream = data.groupby('user')['stream'].min()\n",
    "\n",
    "# --- 2. CALCULATION FUNCTION ---\n",
    "def analyze_arc(arc_name, stream_list, df_chat, df_stats):\n",
    "    # Filter chat data and stats data for just these streams\n",
    "    arc_chat = df_chat[df_chat['stream'].isin(stream_list)]\n",
    "    arc_stats = df_stats[df_stats['stream'].isin(stream_list)]\n",
    "    \n",
    "    # 1. Total Duration (sum of last message - first message per stream)\n",
    "    stream_durations = arc_chat.groupby('stream')['date'].agg(['min', 'max'])\n",
    "    stream_durations['duration'] = stream_durations['max'] - stream_durations['min']\n",
    "    total_duration = stream_durations['duration'].sum()\n",
    "    \n",
    "    # 2. Unique Chatters\n",
    "    unique_chatters = arc_chat['user'].nunique()\n",
    "    \n",
    "    # 3. First-time Chatters (Users whose VERY FIRST message ever falls in this arc)\n",
    "    first_time_chatters = user_first_stream[user_first_stream.isin(stream_list)].count()\n",
    "    \n",
    "    # 4. Average Message Speed (Messages per minute)\n",
    "    total_messages = len(arc_chat)\n",
    "    total_minutes = total_duration.total_seconds() / 60\n",
    "    # Prevent division by zero just in case\n",
    "    avg_msg_speed = (total_messages / total_minutes) if total_minutes > 0 else 0\n",
    "    \n",
    "    # 5. Total Follows\n",
    "    total_follows = arc_stats['follow'].sum()\n",
    "    \n",
    "    # 6. Top 10 Chatters\n",
    "    top_10_chatters = arc_chat['user'].value_counts().head(10)\n",
    "    \n",
    "    # 7. Top 10 Emotes\n",
    "    # Split all messages by space, explode into a single column of words, and drop NaNs\n",
    "    all_words = arc_chat['message'].dropna().astype(str).str.split(expand=False).explode()\n",
    "    # Filter to only keep words that exist in our emote set\n",
    "    emotes_used = all_words[all_words.isin(emote_set)]\n",
    "    top_10_emotes = emotes_used.value_counts().head(10)\n",
    "    \n",
    "    # --- PRINT RESULTS ---\n",
    "    print(f\"=== {arc_name.upper()} ARC ===\")\n",
    "    print(f\"Total Duration: {total_duration}\")\n",
    "    print(f\"Total Messages: {total_messages}\")\n",
    "    print(f\"Unique Chatters: {unique_chatters}\")\n",
    "    print(f\"First-Time Chatters: {first_time_chatters}\")\n",
    "    print(f\"Avg Message Speed: {avg_msg_speed:.2f} messages/min\")\n",
    "    print(f\"Total Follows: {total_follows}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Chatters:\")\n",
    "    print(top_10_chatters.to_string())\n",
    "    \n",
    "    print(\"\\nTop 10 Emotes:\")\n",
    "    print(top_10_emotes.to_string())\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "# --- 3. RUN THE ANALYSIS ---\n",
    "analyze_arc(\"LA Noire (156-166)\", la_noire_streams, data, steamStats)\n",
    "analyze_arc(\"LA Trip (538-558)\", la_streams, data, steamStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d64cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audience Retention: 6.0% of LA Noire chatters returned for the LA Trip.\n",
      "\n",
      "=== LURKER METRICS ===\n",
      "LA Noire: ~419 avg viewers | 3629 unique chatters\n",
      "LA Trip: ~947 avg viewers | 5744 unique chatters\n"
     ]
    }
   ],
   "source": [
    "# --- AUDIENCE RETENTION ---\n",
    "# 1. Get unique users from both arcs\n",
    "la_noire_users = set(data[data['stream'].isin(la_noire_streams)]['user'])\n",
    "la_trip_users = set(data[data['stream'].isin(la_streams)]['user'])\n",
    "\n",
    "# 2. Find the overlap (users who chatted in BOTH)\n",
    "retained_users = la_noire_users.intersection(la_trip_users)\n",
    "\n",
    "# 3. Calculate the retention percentage\n",
    "if len(la_noire_users) > 0:\n",
    "    retention_rate = (len(retained_users) / len(la_noire_users)) * 100\n",
    "    print(f\"Audience Retention: {retention_rate:.1f}% of LA Noire chatters returned for the LA Trip.\")\n",
    "\n",
    "# --- THE LURKER RATIO (Avg Viewers vs. Unique Chatters) ---\n",
    "# Calculate for LA Noire\n",
    "noire_stats = steamStats[steamStats['stream'].isin(la_noire_streams)]\n",
    "noire_avg_viewers = noire_stats['avg'].mean()\n",
    "noire_chatters = len(la_noire_users)\n",
    "\n",
    "# Calculate for LA Trip\n",
    "la_stats = steamStats[steamStats['stream'].isin(la_streams)]\n",
    "la_avg_viewers = la_stats['avg'].mean()\n",
    "la_chatters = len(la_trip_users)\n",
    "\n",
    "print(f\"\\n=== LURKER METRICS ===\")\n",
    "print(f\"LA Noire: ~{noire_avg_viewers:.0f} avg viewers | {noire_chatters} unique chatters\")\n",
    "print(f\"LA Trip: ~{la_avg_viewers:.0f} avg viewers | {la_chatters} unique chatters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57168610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LA NOIRE (156-166) ADVANCED METRICS ===\n",
      "Core Chatter Concentration: The top 10% of chatters sent 82.3% of all messages.\n",
      "Core Chatter Concentration: The top 1% of chatters sent 51.2% of all messages.\n",
      "Average Message Length: 22.9 characters.\n",
      "Follow-to-Peak Conversion Rate: 23.06% avg conversion per stream.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== LA TRIP (538-558) ADVANCED METRICS ===\n",
      "Core Chatter Concentration: The top 10% of chatters sent 86.6% of all messages.\n",
      "Core Chatter Concentration: The top 1% of chatters sent 63.5% of all messages.\n",
      "Average Message Length: 18.8 characters.\n",
      "Follow-to-Peak Conversion Rate: 4.29% avg conversion per stream.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_advanced_metrics(arc_name, stream_list, df_chat, df_stats):\n",
    "    # Filter data for the specific arc using .copy() to prevent SettingWithCopy warnings\n",
    "    arc_chat = df_chat[df_chat['stream'].isin(stream_list)].copy()\n",
    "    arc_stats = df_stats[df_stats['stream'].isin(stream_list)].copy()\n",
    "    \n",
    "    print(f\"=== {arc_name.upper()} ADVANCED METRICS ===\")\n",
    "    \n",
    "    # 1. Core Chatter Concentration (Top 10% of chatters)\n",
    "    user_counts = arc_chat['user'].value_counts()\n",
    "    total_msgs = len(arc_chat)\n",
    "    # Find how many users make up the top 10% (minimum of 1)\n",
    "    top_10_pct_count = max(1, int(len(user_counts) * 0.10))\n",
    "    top_10_msgs = user_counts.head(top_10_pct_count).sum()\n",
    "    concentration = (top_10_msgs / total_msgs) * 100 if total_msgs > 0 else 0\n",
    "    \n",
    "    print(f\"Core Chatter Concentration: The top 10% of chatters sent {concentration:.1f}% of all messages.\")\n",
    "\n",
    "    # 1.5. Core Chatter Concentration (Top 10% of chatters)\n",
    "    \n",
    "    # Find how many users make up the top 10% (minimum of 1)\n",
    "    top_1_pct_count = max(1, int(len(user_counts) * 0.01))\n",
    "    top_1_msgs = user_counts.head(top_1_pct_count).sum()\n",
    "    concentration = (top_1_msgs / total_msgs) * 100 if total_msgs > 0 else 0\n",
    "    \n",
    "    print(f\"Core Chatter Concentration: The top 1% of chatters sent {concentration:.1f}% of all messages.\")\n",
    "    \n",
    "    # 2. Message Length Analysis\n",
    "    # Convert to string and calculate length using pandas vectorized .str.len()\n",
    "    avg_char_length = arc_chat['message'].astype(str).str.len().mean()\n",
    "    print(f\"Average Message Length: {avg_char_length:.1f} characters.\")\n",
    "    \n",
    "    # 3. Follow-to-Peak Conversion Rate\n",
    "    # Temporarily replace any 0 max viewers with 1 to avoid division by zero errors\n",
    "    safe_max = arc_stats['max'].replace(0, 1)\n",
    "    arc_stats['conversion'] = (arc_stats['follow'] / safe_max) * 100\n",
    "    avg_conversion = arc_stats['conversion'].mean()\n",
    "    print(f\"Follow-to-Peak Conversion Rate: {avg_conversion:.2f}% avg conversion per stream.\")\n",
    "    \n",
    "        \n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# --- RUN THE ADVANCED ANALYSIS ---\n",
    "# (Make sure la_noire_streams, la_streams, data, and steamStats are loaded from the previous script)\n",
    "analyze_advanced_metrics(\"LA Noire (156-166)\", la_noire_streams, data, steamStats)\n",
    "analyze_advanced_metrics(\"LA Trip (538-558)\", la_streams, data, steamStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa0ffd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LA NOIRE (156-166) COMMUNITY HEALTH ===\n",
      "Chatter Dedication: The average dedicated chatter (>1 msg) showed up to 4.1 out of 11 streams.\n",
      "Talkativeness Score: 16.9 messages sent per active chatter.\n",
      "Growth Efficiency: Gained 23.2 follows per hour of streaming.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== LA TRIP (538-558) COMMUNITY HEALTH ===\n",
      "Chatter Dedication: The average dedicated chatter (>1 msg) showed up to 5.1 out of 21 streams.\n",
      "Talkativeness Score: 18.5 messages sent per active chatter.\n",
      "Growth Efficiency: Gained 18.7 follows per hour of streaming.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_community_health(arc_name, stream_list, df_chat, df_stats):\n",
    "    # Filter the data for this specific arc\n",
    "    arc_chat = df_chat[df_chat['stream'].isin(stream_list)]\n",
    "    arc_stats = df_stats[df_stats['stream'].isin(stream_list)]\n",
    "    \n",
    "    print(f\"=== {arc_name.upper()} COMMUNITY HEALTH ===\")\n",
    "    \n",
    "    # --- 1. Chatter Dedication (>1 Message Filter) ---\n",
    "    total_arc_streams = len(arc_chat['stream'].unique())\n",
    "    \n",
    "    # Count total messages per user in this arc\n",
    "    user_msg_counts = arc_chat.groupby('user').size()\n",
    "    # Create a list of users who sent more than 1 message\n",
    "    dedicated_users = user_msg_counts[user_msg_counts > 10].index\n",
    "    \n",
    "    # Filter our chat data to ONLY include those dedicated users\n",
    "    dedicated_chat = arc_chat[arc_chat['user'].isin(dedicated_users)]\n",
    "    \n",
    "    # Count how many unique streams each dedicated user appeared in\n",
    "    streams_per_user = dedicated_chat.groupby('user')['stream'].nunique()\n",
    "    \n",
    "    # Prevent errors just in case there's an empty dataset\n",
    "    avg_attendance = streams_per_user.mean() if not streams_per_user.empty else 0\n",
    "    \n",
    "    print(f\"Chatter Dedication: The average dedicated chatter (>1 msg) showed up to {avg_attendance:.1f} out of {total_arc_streams} streams.\")\n",
    "    \n",
    "    # --- 2. Talkativeness Score (Messages per Chatter) ---\n",
    "    # We still use the un-filtered chat here to get the true overall average\n",
    "    unique_chatters = arc_chat['user'].nunique()\n",
    "    total_messages = len(arc_chat)\n",
    "    msgs_per_chatter = total_messages / unique_chatters if unique_chatters > 0 else 0\n",
    "    \n",
    "    print(f\"Talkativeness Score: {msgs_per_chatter:.1f} messages sent per active chatter.\")\n",
    "    \n",
    "    # --- 3. Growth Efficiency (Follows per Hour) ---\n",
    "    # Calculate total duration in hours\n",
    "    stream_durations = arc_chat.groupby('stream')['date'].agg(['min', 'max'])\n",
    "    stream_durations['duration'] = stream_durations['max'] - stream_durations['min']\n",
    "    total_hours = stream_durations['duration'].sum().total_seconds() / 3600\n",
    "    \n",
    "    total_follows = arc_stats['follow'].sum()\n",
    "    follows_per_hour = total_follows / total_hours if total_hours > 0 else 0\n",
    "    \n",
    "    print(f\"Growth Efficiency: Gained {follows_per_hour:.1f} follows per hour of streaming.\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# --- RUN THE HEALTH ANALYSIS ---\n",
    "analyze_community_health(\"LA Noire (156-166)\", la_noire_streams, data, steamStats)\n",
    "analyze_community_health(\"LA Trip (538-558)\", la_streams, data, steamStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "046b47cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LA NOIRE (156-166) CHAT CULTURE & VIBE ===\n",
      "Hype Temperature: 10.9% of messages were screaming in ALL CAPS.\n",
      "Curiosity Index: 8.8% of messages contained a question mark.\n",
      "Dead Air Index: The average 'longest silence' per stream was 72 seconds.\n",
      "  -> (The absolute longest chat silence in this arc was 159 seconds.)\n",
      "Introvert/Extrovert Ratio: For every 1 follow, there were 1.2 first-time chatters.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== LA TRIP (538-558) CHAT CULTURE & VIBE ===\n",
      "Hype Temperature: 12.0% of messages were screaming in ALL CAPS.\n",
      "Curiosity Index: 6.4% of messages contained a question mark.\n",
      "Dead Air Index: The average 'longest silence' per stream was 47 seconds.\n",
      "  -> (The absolute longest chat silence in this arc was 303 seconds.)\n",
      "Introvert/Extrovert Ratio: For every 1 follow, there were 1.5 first-time chatters.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_chat_culture(arc_name, stream_list, df_chat, df_stats, user_first_stream):\n",
    "    # Filter the data for this specific arc\n",
    "    arc_chat = df_chat[df_chat['stream'].isin(stream_list)].copy()\n",
    "    arc_stats = df_stats[df_stats['stream'].isin(stream_list)].copy()\n",
    "    \n",
    "    print(f\"=== {arc_name.upper()} CHAT CULTURE & VIBE ===\")\n",
    "    \n",
    "    # Ensure messages are strings to prevent errors\n",
    "    text_msgs = arc_chat['message'].dropna().astype(str)\n",
    "    \n",
    "    # --- 1. Hype Temperature (Caps Lock Ratio) ---\n",
    "    # Only look at messages 3 characters or longer (ignores \"W\", \"F\", \"L\")\n",
    "    long_msgs = text_msgs[text_msgs.str.len() >= 3]\n",
    "    caps_msgs = long_msgs[long_msgs.str.isupper()]\n",
    "    hype_ratio = (len(caps_msgs) / len(long_msgs)) * 100 if len(long_msgs) > 0 else 0\n",
    "    print(f\"Hype Temperature: {hype_ratio:.1f}% of messages were screaming in ALL CAPS.\")\n",
    "    \n",
    "    # --- 2. Curiosity / Backseating Index ---\n",
    "    # Count messages containing a question mark\n",
    "    question_msgs = text_msgs[text_msgs.str.contains(r'\\?', regex=True)]\n",
    "    question_ratio = (len(question_msgs) / len(text_msgs)) * 100 if len(text_msgs) > 0 else 0\n",
    "    print(f\"Curiosity Index: {question_ratio:.1f}% of messages contained a question mark.\")\n",
    "    \n",
    "    # --- 3. The \"Dead Air\" Index ---\n",
    "    # Sort chronologically, then find the time difference between each consecutive message per stream\n",
    "    arc_chat = arc_chat.sort_values(by=['stream', 'date'])\n",
    "    arc_chat['time_diff'] = arc_chat.groupby('stream')['date'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Find the longest gap for each stream, then average those out\n",
    "    max_gaps = arc_chat.groupby('stream')['time_diff'].max()\n",
    "    avg_max_gap = max_gaps.mean()\n",
    "    absolute_max_gap = max_gaps.max()\n",
    "    print(f\"Dead Air Index: The average 'longest silence' per stream was {avg_max_gap:.0f} seconds.\")\n",
    "    print(f\"  -> (The absolute longest chat silence in this arc was {absolute_max_gap:.0f} seconds.)\")\n",
    "    \n",
    "    # --- 4. Introvert vs Extrovert Ratio ---\n",
    "    first_timers = user_first_stream[user_first_stream.isin(stream_list)].count()\n",
    "    total_follows = arc_stats['follow'].sum()\n",
    "    \n",
    "    if total_follows > 0:\n",
    "        extrovert_ratio = first_timers / total_follows\n",
    "        print(f\"Introvert/Extrovert Ratio: For every 1 follow, there were {extrovert_ratio:.1f} first-time chatters.\")\n",
    "    else:\n",
    "        print(\"Introvert/Extrovert Ratio: 0 follows in this dataset, cannot calculate ratio.\")\n",
    "        \n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# --- RUN THE CULTURE ANALYSIS ---\n",
    "# (Make sure to pass 'user_first_stream' which we calculated in the very first block of code!)\n",
    "analyze_chat_culture(\"LA Noire (156-166)\", la_noire_streams, data, steamStats, user_first_stream)\n",
    "analyze_chat_culture(\"LA Trip (538-558)\", la_streams, data, steamStats, user_first_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d6cc8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LA NOIRE (156-166) FINANCIAL ROI ===\n",
      "Total Investment: $20.00\n",
      "\n",
      "Production Cost: $0.32 per hour of streamed content.\n",
      "Cost per Follower (CAC): $0.01 spent to acquire each new follower.\n",
      "Cost per Active Chatter: $0.01 spent per engaged viewer.\n",
      "Cost per Message: $0.0003 spent for every line of chat.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== LA TRIP (538-558) FINANCIAL ROI ===\n",
      "Total Investment: $10,000.00\n",
      "\n",
      "Production Cost: $173.99 per hour of streamed content.\n",
      "Cost per Follower (CAC): $9.32 spent to acquire each new follower.\n",
      "Cost per Active Chatter: $1.74 spent per engaged viewer.\n",
      "Cost per Message: $0.0944 spent for every line of chat.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_financial_roi(arc_name, stream_list, df_chat, df_stats, total_cost_usd):\n",
    "    # Filter the data for this specific arc\n",
    "    arc_chat = df_chat[df_chat['stream'].isin(stream_list)]\n",
    "    arc_stats = df_stats[df_stats['stream'].isin(stream_list)]\n",
    "    \n",
    "    print(f\"=== {arc_name.upper()} FINANCIAL ROI ===\")\n",
    "    print(f\"Total Investment: ${total_cost_usd:,.2f}\\n\")\n",
    "    \n",
    "    # --- 1. Cost per Hour of Content ---\n",
    "    stream_durations = arc_chat.groupby('stream')['date'].agg(['min', 'max'])\n",
    "    stream_durations['duration'] = stream_durations['max'] - stream_durations['min']\n",
    "    total_hours = stream_durations['duration'].sum().total_seconds() / 3600\n",
    "    \n",
    "    cost_per_hour = total_cost_usd / total_hours if total_hours > 0 else 0\n",
    "    print(f\"Production Cost: ${cost_per_hour:.2f} per hour of streamed content.\")\n",
    "    \n",
    "    # --- 2. Cost per Follower (Customer Acquisition Cost - CAC) ---\n",
    "    total_follows = arc_stats['follow'].sum()\n",
    "    cac = total_cost_usd / total_follows if total_follows > 0 else 0\n",
    "    print(f\"Cost per Follower (CAC): ${cac:.2f} spent to acquire each new follower.\")\n",
    "    \n",
    "    # --- 3. Cost per Active Chatter (Engagement Cost) ---\n",
    "    unique_chatters = arc_chat['user'].nunique()\n",
    "    cost_per_chatter = total_cost_usd / unique_chatters if unique_chatters > 0 else 0\n",
    "    print(f\"Cost per Active Chatter: ${cost_per_chatter:.2f} spent per engaged viewer.\")\n",
    "    \n",
    "    # --- 4. Cost per Message ---\n",
    "    total_messages = len(arc_chat)\n",
    "    cost_per_message = total_cost_usd / total_messages if total_messages > 0 else 0\n",
    "    print(f\"Cost per Message: ${cost_per_message:.4f} spent for every line of chat.\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# --- RUN THE FINANCIAL ANALYSIS ---\n",
    "analyze_financial_roi(\"LA Noire (156-166)\", la_noire_streams, data, steamStats, 20.00)\n",
    "analyze_financial_roi(\"LA Trip (538-558)\", la_streams, data, steamStats, 10000.00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
